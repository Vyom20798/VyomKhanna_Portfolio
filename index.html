<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YWGXGNKDWB"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-YWGXGNKDWB');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vyom Khanna - Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <button id="nav-toggle" class="nav-toggle">☰ Menu</button>
    <div id="nav-drawer" class="nav-drawer">
        <h2>Navigation</h2>
        <li><a href="#home">Home</a></li>
        <li><a href="#about">About Me</a></li>
        <li><a href="#experience">Experience</a></li>
        <li><a href="#skills">Skills</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#education">Education</a></li>
        <li><a href="#contact">Contact</a></li>
        <li><a href="#resume">Resume</a></li>
        <li><a href="#certificates">Certificates</a></li>
        <li><a href="#mobility">Mobility</a></li>
        <li><a href="#right_to_work">Right To Work</a></li>
        <!-- <li><a href="#More">More</a></li> -->
        </ul>
    </div>

    <div class="container">
        <!-- <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#about">About Me</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#contact">Contact</a></li>
                <li><a href="#resume">Resume</a></li>
                <li><a href="#certificates">Certificates</a></li>
                <li><a href="#mobility">Mobility</a></li>
                <li><a href="#right_to_work">Right To Work</a></li>
                <li><a href="#More">More</a></li>
            </ul>
        </nav> -->
        <section id="home">
            <p class="justified-text"><strong>Dear HR / Talent Acquisition / Recruiting Manager,</strong></p>
            <p class="justified-text">Thank you for taking the time to explore my profile.
                I’ve designed this page to give you a clear picture of my skills, qualifications, and (hopefully!) the
                Right to Work details you're seeking. You’ll find everything you need to assess my fit, and a little
                extra to keep things engaging.</p>

            <p class="justified-text">You may have questions, perhaps about my visa status, UK-based experience, or
                alignment with your
                industry. I understand those concerns, and I want to reassure you: my strong work ethic, adaptability,
                and commitment to learning enable me to quickly bridge any gaps. If you're willing to take that initial
                leap of faith, I’ll make it my priority to prove it was the right decision.</p>

            <p class="justified-text">While I’m just 3 years into my career, I bring a solid foundation shaped by
                diverse projects that have
                sharpened my ability to thrive in any environment. I’m confident that within a few months of immersion,
                I can fully adapt to new workflows, office culture, and compliance standards. My background in
                healthcare insurance has instilled in me a deep respect for accuracy, operational integrity, and
                consistently high standards, qualities I apply to every challenge.</p>

            <p class="justified-text">Proactively, I’ve also familiarised myself with the UK GDPR and the Data
                Protection Act 2018. My
                past experiences has reinforced a strong understanding of data confidentiality and
                compliance, principles I carry forward into every professional context.</p>

            <p class="justified-text">I’m excited to explore opportunities across industries, and I’m confident that my
                adaptability,
                attention to detail, and drive to contribute will add real value to your team. If given the chance, I’ll
                bring fresh perspective, dedication, and a commitment to helping your organisation succeed.</p>

            <!-- <p class="justified-text">And if you're searching not only for someone who can deliver data-driven insights
                and strategic solutions but also a teammate who’s up for sharing dog or cat shenanigans, planning a
                Sunday BBQ, swapping life advice, or grabbing a Friday evening drink to debate football highlights, then
                you’re in the right place.</p> -->
            <p class="justified-text">Scroll down, and let’s see if we’re a match!</p>
        </section>

        <section id="about">
            <h2 class="section-title">About Me</h2>
            <!-- Professional Summary Dropdown -->
            <!-- div class="dropdown" style="position: relative;">
      <button class="dropdown-btn">Professional Summary</button>
      <div class="dropdown-content">-->
            <div class="box">
                <p class="justified-text">I’m a Data enthusiast with 3 years of experience across full data lifecycle,
                    backed by an MSc in Data and Decision Analytics from the
                    University of Southampton.</p>
                <p class="justified-text">I work across the full data lifecycle, from extracting and transforming data
                    with SQL and Python, to building automated workflows and modeling complex datasets. I create
                    interactive dashboards using Power BI, Tableau, and Qlik Sense that translate data into meaningful
                    insights for decision-makers.</p>
                <p class="justified-text">My expertise includes developing scalable reporting solutions, trend analysis,
                    KPI monitoring, and process automation, which have significantly reduced manual effort and improved
                    data accessibility. I streamline project workflows using Agile and Waterfall methodologies,
                    supported by tools like Trello, Excel, and stakeholder engagement strategies.</p>
                <p class="justified-text">I have a solid grasp of data privacy and compliance, with practical knowledge
                    of UK GDPR and the Data Protection Act 2018, ensuring all data-handling processes remain secure and
                    compliant.</p>
                <p class="justified-text">Passionate about data storytelling, I enjoy simplifying complex insights for
                    both technical and non-technical audiences. I’m a continuous learner, actively exploring tools like
                    dbt and Google Analytics while leveraging AI and LLMs to stay ahead in the evolving data landscape.
                </p>
                <br>
                <p class="justified-text"><b>Beyond Work:</b></p>
                <p class="justified-text">Outside the office, I’m an energetic and curious individual who loves
                    exploring new ideas and experiences.</p>
                <p class="justified-text">My weekends are often filled with pop music playlists, photography sessions,
                    and some good-natured competition at the arcade or over a barbecue.</p>
                <p class="justified-text">A sports enthusiast at heart, I’m passionate about football and Formula 1
                    cheering for Manchester United on the pitch and Ferrari on the track, so it can be said Red is my
                    favourite color.</p>
                <p class="justified-text">These hobbies keep me inspired, energised, and always ready to bring a fresh
                    perspective to every challenge I tackle.</p>
            </div>
            <!--</div> -->
            <!--</div> -->
            <br>
            <!-- Personal Summary Dropdown -->
            <!-- <div class="dropdown" style="position: relative;">
      <button class="dropdown-btn">Personal Summary</button>
      <div class="dropdown-content">-->
            <!-- </div>-->
            <!-- </div>-->
        </section>

        <section id="experience">
            <h2 class="section-title">Experience</h2>
            <!-- <div class="dropdown" style="position: relative;">
            <button class="dropdown-btn">Poundland and Dealz</button>
            <div class="dropdown-content"> -->
            <div class="box" and id="Poundland">
                <a href="#Poundland" target="_blank"></a>
                <h3><u>Poundland and Dealz, Southampton, UK</u></h3>
                <p class="justified-text"><i>Sales Assistant | April 2024 - | Industry - FMCG</i></p>
                <!-- <p class="justified-text">April 2024 - Present</p>
                <p class="justified-text">Industry - FMCG</p> -->
                <p class="justified-text">
                    <!-- Currently working part-time at this fast-paced FMCG retailer, which I joined during my
                    post-graduation degree, while searching for a full-time role. -->
                    I currently work as a Sales Assistant at Poundland on Above Bar Street in Southampton, which is
                    known to be one of the busiest Poundland locations on the South Coast. This role is part-time and
                    primarily supports my cost of living in the UK, while I dedicate the remainder of my time to
                    upskilling and actively seeking full-time career opportunities.
                </p>
                <p class="justified-text"><b>Key Achievements and Responsibilities:</b></p>
                <ul class="hanging-indent">
                    <!-- <li>
                        <span><strong>Prepared and Monitored Reports:</strong></span>
                        <span>Compiled weekly and monthly SKU reports using sales data from the store to track product
                            trends, basket composition, and customer buying habits. These reports helped the team stay
                            informed about what was working and what needed adjustment on the shop floor.</span>
                    </li>

                    <li>
                        <span><strong>Tableau Dashboards:</strong></span>
                        <span>Created clear and easy-to-use Tableau dashboards to show key metrics like average basket
                            size, top and bottom sellers, and how different products performed at various times of the
                            day, helping staff plan better around peak hours and promotions.</span>
                    </li>

                    <li>
                        <span><strong>Performance and Customer Insights:</strong></span>
                        <span>Shared insights during weekly business reviews with store leadership, offering practical
                            suggestions to improve product visibility and upselling. This often led to smarter shelf
                            arrangements, better product pairing, and increased add-on sales.</span>
                    </li>
 -->
                    <li>
                        <span><strong>Exceeded Sales Targets:</strong></span>
                        <span>Surpassed weekly sales targets by 15% during christmas through strategic customer
                            engagement and tailored product recommendations.</span>
                    </li>

                    <li>
                        <span><strong>Customer Service Excellence:</strong></span>
                        <span>Delivered high-quality service, resulting in positive feedback from customers and
                            recognition from management.</span>
                    </li>
                    <!-- <li>
                        <span><strong>Visual Merchandising Collaboration:</strong></span>
                        <span>Partnered with the visual merchandising team to design eye-catching displays, increasing
                            foot
                            traffic.</span>
                    </li> -->
                    <li>
                        <span><strong>Stock Replenishment and Pricing Accuracy:</strong></span>
                        <span>Ensured timely stock replenishment and accurate pricing for optimal customer
                            experience.</span>
                    </li>
                    <li>
                        <span><strong>Inventory Support:</strong></span>
                        <span>Reduced stock discrepancies by 10% through precise inventory management and protocol
                            adherence.</span>
                    </li>

                    <li>
                        <span><strong>Food and Product Waste Control:</strong></span>
                        <span>Implemented effective waste management practices, reducing food and product wastage by 60%
                            through accurate stock rotation, monitoring expiration dates, and minimising
                            overstocking.</span>
                    </li>
                </ul>

                <!-- <p class="justified-text"><b>KPIs Presented:</b></p>
                <ul class="hanging-indent"> -->
            </div>
            <!--- </div>
            </div> -->
            <!-- <div class="dropdown" style="position: relative;">
            <button class="dropdown-btn">Team Computers</button>
            <div class="dropdown-content"> -->
            <div class="box" and id="Team_Computers">
                <a href="#Team_Computers" target="_blank"></a>
                <h3><u>Team Computers, Gurgaon, India</u></h3>
                <p class="justified-text"><i>Business Intelligence Engineer & Analyst | September 2021 - August 2023 |
                        Industry -
                        Insurance</i></p>
                <!-- <p class="justified-text">September 2021 - May 2023</p>
                <p class="justified-text">Industry - Insurance</p> -->
                <p class="justified-text">In this role, I worked as a Business Intelligence Engineer for a
                    healthcare insurance client, where I focused on leveraging SQL, Python, and BI tools to transform
                    raw data into actionable
                    insights, automate reporting, and streamline decision-making processes. In short I was involved with
                    end-to-end data lifecycle. I can't put everything on the resume so here you'll see I am trying to
                    put in my responsibilities as per data
                    lifecycle sequence.</p>
                <p class="justified-text"><b>Key Achievements and Responsibilities:</b></p>
                <ul class="hanging-indent">
                    <li>
                        <span><strong>ETL (Extract, Transform, Load) Expertise:</strong></span>
                        <span>Engineered scalable ETL workflows using SQL, Alteryx, and DBT to streamline and automate
                            the data ingestion process. Migrated legacy data pipelines from a snowflake schema to a star
                            schema architecture, which improved data accessibility, enhanced accuracy (up to 95%), and
                            decreased dashboard refresh time from 8 hours to 3 hours.
                        </span>
                    </li>

                    <li>
                        <span><strong>Data Integration:</strong></span>
                        <span>Transformed and integrated data from internal systems, APIs, and flat files using Alteryx
                            and SQL. Consolidated disparate datasets into Azure and Snowflake data warehouses to enable
                            centralised reporting and analytics.
                        </span>
                    </li>

                    <li>
                        <span><strong>Root Cause Analysis & Performance Diagnostics:</strong></span>
                        <span>Led investigations into ETL failures, dashboard lags, and pipeline inefficiencies.
                            Implemented variable logic, threshold triggers, and fallback procedures to enhance the
                            reliability of the data ecosystem. Collaborated with the Head of Claims and IT to optimise
                            warehouse and server performance, reducing data retrieval latency by 40%.</span>
                    </li>

                    <li>
                        <span><strong>Optimisation for Accuracy:</strong></span>
                        <span>Tuned SQL queries, Power BI DAX measures, and ETL logic to reduce data latency and improve
                            reliability. Reduced dashboard loading times by 3 hours while maintaining 95%+ data
                            accuracy, enhancing the user experience and trust in data outputs.</span>
                    </li>

                    <li>
                        <span><strong>Excel for Validation and Reporting:</strong></span>
                        <span>Used advanced Excel functions (pivot tables, nested formulas, conditional formatting,
                            VLOOKUP) for ad hoc validation, compliance checks, and fast-turnaround analysis. Excel
                            reports were especially effective for teams without access to enterprise BI platforms,
                            enabling quick, data-backed decision-making.
                        </span>
                    </li>

                    <li>
                        <span><strong>Data SME & Issue Resolution:</strong></span>
                        <span> Served as a Data Subject Matter Expert (SME), actively supporting stakeholders by
                            troubleshooting report discrepancies, clarifying data definitions, and validating KPIs.
                            Built credibility across functions by ensuring high data accuracy and resolving issues
                            quickly, resulting in increased trust and consistent report usage.
                        </span>
                    </li>

                    <li>
                        <span><strong>Data Analysis & Dashboarding:</strong></span>
                        <span>Utilised SQL and Python to perform detailed analysis of healthcare insurance datasets,
                            identifying key business trends, customer behaviour patterns, and operational
                            inefficiencies. Translated complex data into clear, visual insights using QlikSense and
                            Power BI dashboards, enabling stakeholders to make timely, data-informed decisions.</span>
                    </li>

                    <li>
                        <span><strong>Dashboards:</strong></span>
                        <span>Built and maintained cross-functional dashboards that monitored real-time KPIs such as
                            policy uptake, claim cycle times, and financial exposure. These dashboards were used by
                            department heads and the MD to prioritise actions, track goals, and allocate resources. One
                            flagship dashboard for travel insurance helped reduce claim resolution time by 20% and
                            improved customer retention by 15%.</span>
                    </li>

                    <li>
                        <span><strong>Dashboard Lifecycle Management:</strong></span>
                        <span>Managed the full dashboard development lifecycle from scoping to deployment and
                            maintenance. Ensured that business dashboards remained relevant, accurate, and
                            user-friendly, with periodic enhancements based on stakeholder feedback.</span>
                    </li>

                    <li>
                        <span><strong>Prepared and Monitored Reports:</strong></span>
                        <span>Created and maintained daily, weekly, and monthly reports across Claims, Renewals, and
                            Sales. One key outcome was reducing average pending healthcare claims from 80K to 38K,
                            increasing claims processing productivity by 52%.</span>
                    </li>

                    <li>
                        <span><strong>Customer Experience Dashboards:</strong></span>
                        <span>Built CRM dashboards using Power BI to track Net Promoter Score (NPS) and customer service
                            ratings. These visual tools enabled business teams to identify service gaps and implement
                            targeted improvements, leading to measurable increases in customer satisfaction.</span>
                    </li>

                    <li>
                        <span><strong>Automation:</strong></span>
                        <span>Automated over 100 recurring internal and B2B client reports using a combination of
                            Python, Qlik NPrinting, and Qlik QMC. This initiative cut manual reporting effort by 70%,
                            improved accuracy, and ensured timely delivery of business-critical insights, often under
                            tight SLAs.
                        </span>
                    </li>

                    <li>
                        <span><strong>Ad-Hoc Analysis & ETL:</strong></span>
                        <span>Performed real-time analyses by extracting and transforming raw data from multiple sources
                            using SQL, Excel, Python, QlikSense, and Power BI. Addressed unique business questions
                            ranging from claim backlogs to fraud detection and customer segmentation.</span>
                    </li>

                    <li>
                        <span><strong>Advanced Analytics Enablement:</strong></span>
                        <span>Centralised disparate data sources into Azure and Snowflake warehouses to support
                            higher-level analytics use cases, including predictive modelling, segmentation, and
                            forecasting. Prepared structured datasets that could be directly used for machine learning
                            or scenario simulations.</span>
                    </li>

                    <li>
                        <span><strong>Predictive Modeling & Risk Scoring:</strong></span>
                        <span>Built regression and classification models in Python (Scikit-learn, Pandas, Seaborn) to
                            predict high-risk claims and support fraud detection. These models empowered risk teams to
                            take proactive actions, reducing false claims exposure and enhancing underwriting
                            accuracy.</span>
                    </li>

                    <li>
                        <span><strong>Client Needs Assessment:</span></strong>
                        <span>Interpreted and translated complex client requirements into meaningful analytical outputs.
                            Delivered custom dashboards, ad-hoc reports, and visual tools that directly supported
                            decision-making, performance evaluation, and operational planning.</span>
                    </li>

                    <!-- <li>
                        <span><strong>Sales Impact:</strong></span>
                        <span>Developed data models and dashboards to analyse sales trends, customer demographics, and
                            marketing campaign effectiveness. This enabled smarter targeting strategies and contributed
                            to a 20% increase in online policy sales.</span>
                    </li> -->

                    <li>
                        <span><strong>Incentive Reporting & Sales Impact:</strong></span>
                        <span>Created detailed performance tracking reports for the sales team to link individual
                            performance with incentives and goals. These dashboards helped increase transparency and
                            accountability, ultimately contributing to a 15% boost in quarterly sales productivity.
                            Additionally, built customer segmentation dashboards that supported a 20% uplift in online
                            policy sales through targeted campaigns.
                        </span>
                    </li>

                    <li>
                        <span><strong>Client Collaboration:</strong></span>
                        <span>Worked alongside cross-functional teams, including Claims, IT, Finance, and Sales, to
                            define reporting goals and business KPIs. Participated in Agile sprints and traditional
                            Waterfall cycles to deliver BI solutions that were aligned with real business needs. Reduced
                            ad hoc reporting load by 25% by delivering standardised self-serve dashboards.
                        </span>
                    </li>

                    <li>
                        <span><strong>Stakeholder Collaboration:</strong></span>
                        <span>Worked collaboratively with the Head of Claims and IT infrastructure teams to troubleshoot
                            performance issues, leading to a 20% improvement in server response time and enhanced
                            dashboard responsiveness.
                        </span>
                    </li>

                    <li>
                        <span><strong>Training & Stakeholder Support:</strong></span>
                        <span> Led structured training sessions for business users, analysts, and department leads to
                            drive adoption of Power BI and Qlik dashboards. Developed supporting documentation and
                            walkthroughs, significantly improving data literacy across teams and reducing reliance on
                            manual reports and IT support.
                        </span>
                    </li>

                    <li>
                        <span><strong>Mentorship and Training:</strong></span>
                        <span> Trained junior developers and interns by providing hands-on guidance in data analysis
                            techniques, SQL query optimisation, and Python-based automation. This fostered a
                            collaborative environment and accelerated their learning curve, improving overall team
                            efficiency.
                        </span>
                    </li>

                </ul>
                <br>
                <!-- <p class="justified-text"><b>Requirement-Gathering Process My structured approach to requirement
                        gathering ensures precision
                        and
                        alignment across all project phases:</b></p>
                <ul class="hanging-indent">
                    <li><strong>Organise a Meeting:</strong>I begin by meeting with users or stakeholders to gather
                        detailed
                        requirements, discussing alternative approaches that may improve process efficiency.</li>
                    <li><strong>Detailed Documentation:</strong> I document all requirements thoroughly, specifying data
                        sources, tables, primary keys, and any supporting materials (e.g., Excel files, previous
                        reports).
                    </li>
                    <li><strong>Reach Agreement:</strong> Once aligned with stakeholders, I set a timeline and deadlines
                        to
                        ensure timely delivery.</li>
                    <li><strong>Data Extraction and Transformation:</strong> I extract necessary data using SQL, join
                        relevant tables, and map additional data from master files, ensuring consistency across
                        datasets.
                    </li>
                    <li><strong>Data Cleaning and Validation:</strong> I clean data using SQL techniques (e.g., TRIM,
                        DISTINCT) to eliminate duplicates and ensure data accuracy.</li>
                    <li><strong>Create and Validate Final Output:</strong> I create the final dataset, apply necessary
                        filters, validate all calculations, and coordinate with the user for User Acceptance Testing
                        (UAT)
                        before finalising.</li>
                    <li><strong>Summary and Handover:</strong> After UAT sign-off, I present a summary to stakeholders
                        and
                        pass the final report to the team lead for production.</li>
                </ul>
                <br>
                <b>An example of key delivery</b>
                <p class="justified-text">Claims Pendency Report Automation</p>
                <p class="justified-text">A notable project was automating a comprehensive claims pendency report for
                    the claims team, focused
                    on
                    cases in various stages (e.g., legal disputes, doctor reviews). With a tight 10-day deadline, I
                    segmented claims data into categories (Cashless and Reimbursement) and further broke down each
                    category
                    by type, such as Legal/Disputed, Third-Party Assurance, and Product Categories.</p>
                <ul class="hanging-indent">
                    <li><strong>Process:</strong>Calculated claim aging for each case type, identified closed cases with
                        outstanding amounts, and filtered disputed cases. Completed data processing and organised a UAT
                        session within seven days.</li>
                    <li><strong>Automation:</strong> After UAT approval, I automated the report distribution,
                        designating
                        the Claims HOD as supervisor.</li>
                    <li><strong>Insights Provided:</strong> The final report included claim counts, pending amounts, and
                        reversal cases, along with a monthly productivity chart tracking closed cases.</li>
                    <li><strong>Recognition:</strong> This project earned me a Letter of Recommendation from the Head of
                        Claims at Care Health Insurance, acknowledging my dedication, technical expertise, and impact on
                        the
                        Claims department.</li>
                </ul> -->
            </div>
            <!--- </div>
            </div> -->
            <!--  <div class="dropdown" style="position: relative;">
            <button class="dropdown-btn">Nipa International Ltd</button>
            <div class="dropdown-content"> -->
            <div class="box" and id="Nipa_International_Ltd">
                <a href="Nipa_International_Ltd" target="_blank"></a>
                <h3><u>Nipa International Ltd, Gurgaon, India</u></h3>
                <p class="justified-text"><i>Junior Data Analyst | August 2020 - August 2021 | Indsutry -
                        Manufacturing</i></p>
                <!-- <p class="justified-text">January 2021 - August 2021</p>
                <p class="justified-text">Indsutry - Manufacturing</p> -->
                <p class="justified-text">In this role, I managed work orders from the R&D department, led initiatives
                    to enhance productivity, and implemented data-driven strategies to streamline operations and improve
                    production efficiencies.</p>
                <p class="justified-text"><b>Key Achievements and Responsibilities:</b></p>
                <ul class="hanging-indent">
                    <li><strong>Data Prepration:</strong>Extracted raw manufacturing and operational data from the ERP
                        system using MySQL. Applied data cleaning techniques to standardise entries, correct anomalies,
                        and ensure integrity, resulting in a 20% reduction in data processing time and significantly
                        improving the reliability of downstream analysis.</li>
                    <li><strong>Operational Reporting & Performance Tracking:</strong>Developed and maintained
                        comprehensive performance reports in Excel to monitor critical metrics such as production
                        efficiency, machine downtime, and product defect rates. These reports empowered floor managers
                        to identify bottlenecks and implement timely interventions, leading to a 15% improvement in
                        process tracking and corrective action response.</li>
                    <li><strong>Audit & Compliance Reporting:</strong> Created standardised reporting templates and
                        dashboards tailored for internal and external audits. This initiative reduced audit preparation
                        time by 30% by ensuring consistent data formats, improving traceability, and minimising
                        last-minute data reconciliation efforts.</li>
                    <!-- <li><strong>Database Management:</strong> Updated and maintained employee and tool databases using
                        MySQL, ensuring data consistency and availability for operational needs.</li> -->
                    <li><strong>Data Analysis & Reporting:</strong> Conducted in-depth analysis of raw material
                        consumption and waste patterns across multiple production lines. Identified inefficiencies and
                        areas of overuse that translated into potential annual cost savings of approximately ₹2.5
                        million. Recommendations were shared with procurement and production teams to guide optimisation
                        strategies.</li>
                    <li><strong>Cross-functional Support for Process Optimisation:</strong> Supported plant operations
                        by providing
                        timely data insights that improved production planning and reduced unplanned downtime,
                        contributing to smoother workflows and better resource utilisation.</li>
                    <li><strong>Inventory & Supplier Insights:</strong> Assisted in analysing stock levels, reorder
                        frequency, and supplier performance using ERP exports. Provided visual summaries and reports
                        that helped procurement teams make informed sourcing decisions, reduce overstock risks, and
                        improve inventory turnover ratios. </li>
                    <!-- <li><strong>Siemens Efficiency Enhancement:</strong> Optimised the Siemens home appliance production
                        line, achieving a 20% increase in efficiency through strategic supply chain adjustments and
                        design improvements.</li>
                    <li><strong>Supplier Negotiations:</strong> Used data insights to inform supplier negotiations,
                        reducing procurement costs by 18% and strengthening relationships with key suppliers.</li> -->
                    <!-- <li><strong>Production Process Improvements:</strong> Increased production efficiency by 15% through
                        Power BI and Excel reports that identified process refinements, ultimately contributing to
                        smoother and more cost-effective operations. Monitored sales trends and B2B purchasing
                        behaviour,
                        providing actionable insights that increased upselling opportunities by 12%.</li> -->
                </ul>
            </div>
            <!--  </div>
            </div> -->
            <!-- <div class="dropdown" style="position: relative;">
            <button class="dropdown-btn" >Commercial Data Analyst - Intern</button>
            <div class="dropdown-content"> -->
            <!-- <div class="box" and id="IP_Infra">
                <a href="IP_Infra" target="_blank"></a>
                <h3><u>Commercial Data Analyst - Intern</u></h3>
                <p class="justified-text">August 2020 - February 2021</p>
                <p class="justified-text">IP Infra</p>
                <p class="justified-text">As a Commercial Data Analyst Intern at IP Infra, I supported management
                    with data insights and
                    forecasting to drive strategic decision-making and optimise pricing.
                <p class="justified-text"><b>Key Achievements and Responsibilities:</p></b>
                <ul class="hanging-indent">
                    <li><strong>Dashboard & Reporting:</strong> Developed weekly and monthly Tableau dashboards to track
                        sales performance, customer acquisition, and churn metrics, delivering insights that informed
                        key
                        strategic decisions.</li>
                    <li><strong>Sales Forecasting:</strong> Analysed sales trends and patterns using SQL and Excel,
                        achieving high forecasting accuracy that contributed to an optimised pricing strategy and better
                        market alignment.</li>
                    <li><strong>Data-Driven Strategy:</strong> Leveraged data insights to support the commercial team,
                        aligning reporting with business goals to enhance operational and financial performance.</li>
                </ul>
            </div> -->
            <!--  </div>
            </div> -->
        </section>

        <section id="skills">
            <h2 class="section-title">Skills</h2>

            <!-- Technical Skills Section -->
            <div class="skills-group">
                <h4>Technical Skills</h4>
                <p class="skills-intro">Technical arsenal I possess</p>
                <div class="skills-inline">
                    <img src="Documents/Logos/python.png" alt="Python logo" class="skill-logo">
                    <img src="Documents/Logos/mysql.png" alt="MySQL logo" class="skill-logo">
                    <img src="Documents/Logos/postgresql.png" alt="Postgre logo" class="skill-logo">
                    <img src="Documents/Logos/excel.png" alt="Excel logo" class="skill-logo">
                    <img src="Documents/Logos/power_bi.png" alt="Power BI logo" class="skill-logo">
                    <img src="Documents/Logos/tableau.png" alt="Tableau logo" class="skill-logo">
                    <img src="Documents/Logos/data_modelling.png" alt="Data Modelling logo" class="skill-logo">
                    <img src="Documents/Logos/visual_studio.png" alt="Visual Studio logo" class="skill-logo">
                    <img src="Documents/Logos/visual_studio_code.png" alt="Visual Studio Code logo" class="skill-logo">
                    <img src="Documents/Logos/trello.svg" alt="Trello" class="skill-logo">
                    <img src="Documents/Logos/alteryx.svg" alt="Alteryx" class="skill-logo">
                    <img src="Documents/Logos/aws.svg" alt="AWS" class="skill-logo">
                    <img src="Documents/Logos/azure.svg" alt="Azure" class="skill-logo">
                    <img src="Documents/Logos/chatgpt.svg" alt="ChatGPT" class="skill-logo">
                    <img src="Documents/Logos/google_analytics.svg" alt="Google Analytics" class="skill-logo">
                    <img src="Documents/Logos/qlik.svg" alt="Qlik" class="skill-logo">
                    <!-- <img src="Documents/Logos/r_lang.svg" alt="R" class="skill-logo"> -->
                </div>
                <ul class="skills-list">
                    <li><strong>SQL:</strong> Windows Functions, CASE, Joins, Indexing,
                        Subquery, Partition by, Aggregate Function, ETL, CTEs,
                        Limit, Rank, Offset, DBT</li>
                    <li><strong>Python:</strong> NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn</li>
                    <li><strong>R:</strong> ggplot2, dplyr, caret, tidyr</li>
                    <li><strong>Power BI:</strong> DAX, Power Query, Drill-through, Slicers, Publisher</li>
                    <li><strong>Qlik Sense / QlikView:</strong> Set Analysis, NPrinting, Resident, Applymap, QMC</li>
                    <li><strong>Scikit-learn:</strong> Linear Regression, Random Forest, KMeans
                    </li>
                    <li><strong>Project Management:</strong> Trello, Agile Methodology,
                        Waterfall Methodology</li>
                    <li><strong>Cloud and warehouse:</strong> Azure, Snowflake</li>
                    <li><strong>Excel:</strong> Pivot Tables, VLOOKUP, Power Query, Solver,
                        VBA, Macros</li>
                    <li><strong>Google Analytics:</strong> Real-time Analytics, User Behaviour
                        Analysis, Audience Insights</li>
                    <li><strong>AI & LLM Tools:</strong> ChatGPT,Copilot
                    </li>
                    <li><strong>Digital Analytics:</strong> A/B Testing, Hypothesis Testing</li>
                </ul>
            </div>

            <!-- Soft Skills Section -->
            <div class="skills-group">
                <h4>Soft Skills</h4>
                <p class="skills-intro">Soft Skills That Power My Work</p>
                <ul class="skills-list">
                    <li><strong>Communication:</strong> Turning data findings into accessible, actionable insights that
                        stakeholders actually want to read.</li>
                    <li><strong>Presentation:</strong> Turning complex data stories into clear, confident
                        presentations, engaging stakeholders, simplifying decisions, and leaving a lasting impact in
                        every room.</li>
                    <li><strong>Problem-Solving:</strong> Finding creative solutions that turn data challenges into
                        opportunities for business growth.</li>
                    <li><strong>Adaptability:</strong> Quickly shifting gears in fast-paced environments, always ready
                        to embrace a new direction.</li>
                    <li><strong>Time Management:</strong> Ensuring projects stay on track and deadlines are met, even in
                        busy, data-heavy weeks.</li>
                    <li><strong>Collaboration:</strong> Bringing enthusiasm and focus to teamwork, supporting team goals
                        while contributing my best.</li>
                    <li><strong>The Precision Experts:</strong> Attention to Detail & Analytical Thinking – Masters of
                        spotting the small stuff and making sense of the big picture.</li>

                </ul>
            </div>
        </section>

        <section id="projects">
            <h2 class="section-title">Projects and Continuous Learning:</h2>
            <p class="justified-text">Throughout my academic and professional journey,
                I’ve actively engaged in diverse projects to sharpen my skills and
                stay updated with the latest technologies.
                These experiences have not only strengthened my technical expertise
                but also equipped me with real-world problem-solving capabilities.
                Here are a few highlights:
            </p>

            <div class="project-navigation">
                <a href="#Claim_Pendency" class="project-nav-box">Company Project 1: Claims Pendency Report for
                    Healthcare Insurance Client</a>
                <a href="#Travel_Dashboard" class="project-nav-box">Company Project 2: Monthly Travel Insurance
                    Dashboard for Stakeholders</a>
                <a href="#B2b_and_Internal_Reporting" class="project-nav-box">Company Project 3: Claims Summary &
                    Compliance
                    Reporting for B2B Partners and Brokers</a>
                <a href="#MD_Reports" class="project-nav-box">Company Project 4: Daily Business Reporting</a>
                <a href="#CRM_Dashboard" class="project-nav-box">Company Project 5: CRM Dashboard – Customer
                    Satisfaction & Service KPIs</a>
                <a href="#Repeated_Claims_Detection" class="project-nav-box">Company Project 6: Repeated Claims
                    Detection – Fraud Prevention</a>
                <a href="#Sales_Incentive_Targets" class="project-nav-box">Company Project 7: Sales Incentive
                    Dashboard</a>
                <a href="#Snowflake_to_Star_Schema" class="project-nav-box">Company Project 8: Data Warehouse
                    Optimisation — Snowflake to Star Schema Migration</a>
                <a href="#Automated_Extraction" class="project-nav-box">Self Project 1: Kaggle Extractor</a>
                <a href="#Python_ETL_PBI_Visualisation_ML" class="project-nav-box">Self Project 2: Monthly Beverage
                    Sales Dashboard – Power BI + Python</a>
                <a href="#Predictive_Model" class="project-nav-box">Project 1: Bike Usage Prediction</a>
                <a href="#Power_BI" class="project-nav-box">Project 2: Retail Analytics</a>
                <a href="#project3" class="project-nav-box">Project 3: NHS Staff Scheduling</a>
                <a href="#project4" class="project-nav-box">Project 4: EV Transition</a>
                <a href="#project5" class="project-nav-box">Project 5: Path Optimisation</a>
                <a href="#project6" class="project-nav-box">Project 6: PCB Assembly Optimisation</a>
                <a href="#project7" class="project-nav-box">Project 7: Covid Impact on UK (Research)</a>
                <a href="#project8" class="project-nav-box">Project 8: Customer Sentiment Analysis</a>
                <a href="#project9" class="project-nav-box">Project 9: Research Paper</a>
            </div>
            <!-- Project 1: Predicting London Bike Usage -->
            <!--<div class="dropdown" style="position: relative;">
            <button class="dropdown-btn">Predicting London Bike Usage Using Machine Learning</button>
            <div class="dropdown-content"> -->

            <div id="Claim_Pendency" class="box">
                <h3><u>Company Project 1: Claims Pendency Report for Healthcare Insurance Client</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The project aimed to improve claims processing efficiency by developing a
                    Claims Pendency Report to track and analyse unresolved claims. The goal was to provide data-driven
                    insights into pending claims, aging trends, and department-wise case distribution, ultimately
                    helping the organisation reduce backlog and enhance productivity.</p>
                <ol class="hanging-indent">
                    <li><strong>Exploratory Data Analytics:</strong>Identified bottlenecks in claims processing by
                        analysing pendency trends, aging, and outstanding claim amounts.</li>
                    <li><strong>Business Intelligence Model:</strong>Created a daily claims pendency report with
                        automated tracking, productivity metrics, and department-wise breakdowns, supporting data-driven
                        decision-making.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection and Processing</strong>
                        <ul>
                            <li>Extracted claims pendency register data from warehouse using SQL, filtering out
                                cases with:
                                <ol>Net amount = 0, reversal amounts, or already paid cases still appearing as
                                    pending.</ol>
                            </li>
                            <li>Segmented claims based on departments:
                                <ol>Third-Party Administrators (TPA)</ol>
                                <ol>Cashless</ol>
                                <ol>Reimbursement</ol>
                                <ol>Pending at Medical</ol>
                                <ol>Travel</ol>
                                <ol>Critical Illness</ol>
                                <ol>Health Checkups</ol>
                            </li>
                            <li>Calculated aging metrics:
                                Defined aging as (Month-end of current month - Last document received date).</li>
                        </ul>
                    </li>

                    <li><strong>Data Preprocessing and Feature Engineering</strong>
                        <ul>
                            <li>Transformed raw data into structured business insights aligned with agile methodology.
                            </li>
                            <li>Created data dumps, summaries showing number of pending cases and total amount pending,
                                and visualisation dashboards to provide decision-makers
                                with clear insights.</li>
                            <li>Developed a monthly productivity report showcasing team performance.</li>
                        </ul>
                    </li>

                    <li><strong>Report Development and Insights</strong>
                        <ul>
                            <li>Designed daily claims pendency reports, summarising:
                                <ol>Total Department wise cases and pending amounts</ol>
                                <ol>Aging trends to identify long-pending claims</ol>
                                <ol>Department-wise claims distribution for
                                    targeted resolution.</ol>
                            </li>
                            <li>Implemented a monthly productivity dashboard, tracking:
                                <ol>Resolved vs pending claims</ol>
                                <ol>Claims processed per team and efficiency improvements.</ol>
                            </li>
                            <li>Delivered reports to Claims HOD, Medical Head, TPA Head, Cashless Head, Reimbursement
                                Head, and their respective teams.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Impact on Claims Processing</strong>
                        <ul>
                            <li>Average Daily Pending claims reduced from 80,000 to 38,000 over 18 months (including new
                                cases).</li>
                            <li>Productivity increased by approximately 52%, improving claims resolution speed.</li>
                        </ul>
                    </li>

                    <li><strong>Business Insights and Optimisation</strong>
                        <ul>
                            <li>The dashboard enabled Aging analysis, helping identify bottlenecks in long-pending
                                cases. Departmental performance tracking, facilitating resource allocation and backlog
                                reduction.</li>
                            <li>The report became a critical tool for tracking claims efficiency and ensuring faster
                                settlements.</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <div id="Travel_Dashboard" class="box">
                <h3><u>Company Project 2: Monthly Travel Insurance Dashboard for Stakeholders</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The project aimed to provide travel insurance stakeholders with a monthly
                    dashboard summarising sales performance, premium trends, policy renewals, and claims processing
                    insights. The goal was to track key business metrics, identify trends, and optimise operational
                    strategies to improve sales, renewal rates, and claims resolution efficiency.</p>
                <ol class="hanging-indent">
                    <li><strong>Exploratory Data Analytics:</strong>Analysed travel insurance sales trends, premium
                        revenue, policy renewal patterns, and claims efficiency.</li>
                    <li><strong>Business Intelligence Model:</strong>Developed an interactive dashboard to visualise
                        sales channels, premium per product, customer retention, and claims turnaround time, aiding in
                        data-driven decision-making.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection and Processing</strong>
                        <ul>
                            <li>Extracted sales, renewal, premium, and claims data using SQL from multiple sources.</li>
                            <li>Segmented data into key categories:
                                <ol>Sales Breakdown: Online vs. Agent vs. Corporate policies.</ol>
                                <ol>Premium Revenue: Total premium collected, premium per product, and revenue trends.
                                </ol>
                                <ol>Policy Renewals: First-time renewals vs. Repeat renewals, renewal rates.</ol>
                                <ol>New vs. Renewal Policies: Comparison of new policy sales vs. renewed policies.</ol>
                                <ol>Claims Processing: Approved, rejected, and pending claims with financial impact
                                    analysis.</ol>
                            </li>
                        </ul>
                    </li>
                    <li><strong>Data Preprocessing and Feature Engineering</strong>
                        <ul>
                            <li>Created derived metrics for trend analysis:
                                <ol>Premium per policy type = (Total premium collected / Number of policies sold).</ol>
                                <ol>Renewal rate = (Number of renewed policies / Total expiring policies).</ol>
                                <ol>New Policy Rate = (New policies sold / Total policies issued).</ol>
                                <ol>Claim settlement time = (Approval date - Claim initiation date).</ol>
                                <ol>Outstanding claim ratio = (Pending claims / Total claims).</ol>
                            </li>
                            <li>Implemented automated data normalisation and visualisation updates for real-time
                                tracking.</li>
                        </ul>
                    </li>

                    <li><strong>Dashboard Development and Insights</strong>
                        <ul>
                            <li>Sales Performance:
                                <ol>Total policies sold, categorised by Online, Agent, Corporate.</ol>
                                <ol>Region-wise policy distribution and trends.</ol>
                                <ol>New vs. Renewal Policies to track business growth.</ol>
                            </li>
                            <li>Premium Revenue Analysis:
                                <ol>Total premium collected from sales and renewals.</ol>
                                <ol>Premium per product category, comparing different policy types.</ol>
                                <ol>Renewal premium vs. New policy premium to assess retention performance.</ol>
                            </li>
                            <li>Policy Renewal Analysis:
                                <ol>Monthly renewal percentage vs. policy lapse rates.</ol>
                                <ol>Trends in repeat renewals vs. first-time renewals.</ol>
                            </li>
                            <li>Claims Processing Insights:
                                <ol>Number of claims filed, approved, rejected, and pending.</ol>
                                <ol>Approval vs. Rejection Rates with financial impact analysis.</ol>
                                <ol>Average Claim Settlement Time and department-wise backlog insights.</ol>
                            </li>
                            <li>Trend Analysis:
                                <ol>Month-over-month growth in policies sold, premium revenue, and claims.</ol>
                                <ol>Seasonality insights (e.g., peak travel months leading to higher policy sales and
                                    claims).</ol>
                            </li>
                            <li>Financial Insights:
                                <ol>Claim payouts vs. premium revenue to assess profitability and risk exposure.</ol>
                            </li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Business Performance Insights</strong>
                        <ul>
                            <li>Provided a consolidated view of travel insurance operations, eliminating reliance on
                                multiple disconnected reports.</li>
                            <li>Identified sales peaks and renewal trends, leading to a 15% increase in customer
                                retention.</li>
                            <li>Enabled tracking of premium revenue growth per product, helping in strategic pricing
                                decisions.</li>
                        </ul>
                    </li>

                    <li><strong>Claims Processing Optimisation</strong>
                        <ul>
                            <li>Reduced claim settlement time by 20%, improving customer satisfaction.</li>
                            <li>Highlighted pending claims backlog, leading to better resource allocation and faster
                                resolutions.</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <div id="B2b_and_Internal_Reporting" class="box">
                <h3><u>Company Project 3: Claims Summary & Compliance Reporting for B2B Partners and Brokers</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The objective of this project was to develop a privacy-compliant, automated
                    reporting solution that provided group policy claims data to external B2B partners and insurance
                    brokers. The solution ensured transparency in claims processing while strictly adhering to IRDAI
                    regulations by eliminating personal health information (PHI) and enabling informed decision-making
                    through key performance metrics and workflow insights.</p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Filtering and Compliance</strong></li>
                    <ul>
                        <li>To ensure regulatory compliance, the project involved filtering the active group policy
                            dataset from the ITD claims register with the following actions:
                            Excluded all personally identifiable information (PII) including:
                            <ol>Insured names</ol>
                            <ol>Contact details</ol>
                            <ol>Residential addresses</ol>
                            <ol>Disease or diagnosis-related data</ol>
                        </li>
                        <li>Retained only a unique anonymised Insured Member ID, fully compliant with IRDAI guidelines.
                            This step was critical in balancing stakeholder transparency with data privacy and security
                            regulations.
                        </li>
                    </ul>
                    <li>
                        <strong>Report Generation and Metrics</strong>
                        <ul>
                            <li>
                                The processed dataset was used to generate a structured PDF report that included both
                                summary-level insights and raw data. The report consisted of:
                                <ul>
                                    <li>
                                        Claims Volume Overview
                                        <ul>
                                            <ol>Number of claims Paid</ol>
                                            <ol>Number of claims Rejected</ol>
                                            <ol>Number of claims Outstanding</ol>
                                        </ul>
                                    </li>
                                    <li>
                                        Outstanding Claim Status Breakdown
                                        <ul>
                                            <ol>Claims pending at Data Entry</ol>
                                            <ol>Claims pending with Medical Team</ol>
                                            <ol>Payment Made pending at Bank</ol>
                                        </ul>
                                    </li>
                                    <li>
                                        Claim Type Analysis
                                        <ul>
                                            <ol>Cashless vs Reimbursement claims</ol>
                                            <ol>Total claim count and amounts under each type</ol>
                                        </ul>
                                    </li>
                                    <li>
                                        Financial Overview
                                        <ul>
                                            <ol>Aggregate financials for each claim type and status</ol>
                                        </ul>
                                    </li>
                                    <li>
                                        Key Performance Indicators (KPIs)
                                        <ul>
                                            <ol>Claim Settlement Ratio (%) = (Number of Claims Settled / Total Claims
                                                Received) × 100</ol>
                                            <ol>Average Claim Processing Time (Days) = Total Time Taken to Settle Claims
                                                / Number of Claims Settled</ol>
                                            <ol>Claim Rejection Rate (%) = (Number of Claims Rejected / Total Claims
                                                Processed) × 100</ol>
                                            <ol>Fraudulent Claims Identified (%) = (Fraudulent Claims / Total Claims
                                                Processed) × 100</ol>
                                        </ul>
                                    </li>
                                    <li>
                                        Pending Claims Ageing Analysis
                                        <ul>
                                            <ol>0–7 days</ol>
                                            <ol>8–15 days</ol>
                                            <ol>16–30 days</ol>
                                            <ol>30+ days</ol>
                                        </ul>
                                    </li>

                                    <li>
                                        Raw Data Dump
                                        <ul>
                                            <ol>An Excel file accompanied the PDF, intended for
                                                internal validation, audits, and deeper analysis by stakeholders.</ol>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </li>

                    <li><strong>Data Processing & Transformation</strong>
                        <ul>
                            <li>Parsed and filtered claim-level data using scripting and logic-based rules to format it
                                into business-ready tables and summaries.</li>
                            <li>Applied aggregation logic to compute KPIs and classification logic for ageing analysis.
                            </li>
                        </ul>
                    </li>

                    <li>
                        <strong>Automation</strong>
                        <ul>
                            <li>Developed a batch file-based automation pipeline to:</li>
                            <ol>Parsed and filtered claim-level data using scripting and logic-based rules to format it
                                into business-ready tables and summaries.</ol>
                            <ol>Applied aggregation logic to compute KPIs and classification logic for ageing analysis.
                            </ol>
                        </ul>
                    </li>

                    <li>
                        <strong>Secure Distribution</strong>
                        <ul>
                            Reports were made available on an external portal accessed by B2B partners and brokers.
                            Controlled access ensured data confidentiality and stakeholder-specific access rights.
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Compliance and Trust</strong>
                        <ul>
                            <li>The reports helped external partners gain operational visibility into claims performance
                                without compromising on personal data, fully aligning with IRDAI compliance standards.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Operational Efficiency</strong>
                        <ul>
                            <li>By automating the data processing and delivery mechanism, the initiative eliminated
                                manual reporting overhead, reduced turnaround time, and increased reporting accuracy.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Strategic Impact</strong>
                        <ul>
                            <li>Provided stakeholders with real-time claim health dashboards and ageing analysis.</li>
                            <li>Enabled brokers and partners to track claim settlements, address delays, and proactively
                                coordinate on pending cases.</li>
                            <li>Strengthened client relationships through transparency and consistent communication.
                            </li>
                        </ul>
                        The reporting framework became a key part of the claims ecosystem, supporting both regulatory
                        needs and business performance tracking.
                    </li>
                </ol>
            </div>

            <div id="MD_Reports" class="box">
                <h3><u>Company Project 4: Daily Business Reporting</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The goal of this project was to create a comprehensive daily reporting system
                    for the Managing Director and Heads of Departments (Claims, Finance, Sales, and Actuarial). The
                    report provided real-time visibility into Daily and Month-To-Date (MTD) performance across key
                    verticals, supporting agile decision-making and deeper accountability.
                </p>

                <h3>Scope and Stakeholders</h3>
                <p class="justified-text">The reporting system catered to the following departments and stakeholder:</p>
                <ol>
                    <li>Managing Director (MD)</li>
                    <li>Department Heads of Claims, Finance, Sales, and Actuarial</li>
                </ol>
                <p>All reports were designed with data integrity, KPI alignment, and executive usability in mind. All
                    data sources were linked using Policy Number + Policy Start Date as a composite identifier across
                    modules.
                </p>

                <h3>Module 1: Claims Performance Dashboard</h3>
                <ol>
                    <li><strong>KPIs tracked</strong></li>
                    <ul>
                        <li>Claim Ratio (%) = (Total Claims Paid / Total Premium Received) × 100</li>
                        <li>Number of Claims Settled (Daily & MTD)</li>
                        <li>Pending Claims Count</li>
                        <li>High-Value Claims: Flagged all claims > ₹1,000,000</li>
                        <li>New Claims Registered (Daily)</li>
                    </ul>
                    <li>
                        <strong>Insights Delivered:</strong>
                        <ul>
                            <li>Claims backlog trend</li>
                            <li>Daily progress on high-value cases</li>
                            <li>Monitoring claim approval efficiency</li>
                        </ul>
                    </li>
                </ol>

                <h3>Module 2: Financial Overview</h3>
                <ol>
                    <li><strong>KPIs tracked</strong></li>
                    <ul>
                        <li>Revenue vs Claims Paid</li>
                        <li>Product-wise Policy Profitability</li>
                        <li>Expense Ratio (%) = (Total Operating Expenses / Net Premium Earned) × 100</li>
                        <li>Broker Commission Payouts</li>
                    </ul>
                    <li>
                        <strong>Insights Delivered:</strong>
                        <ul>
                            <li>Net profitability per product line</li>
                            <li>Financial leakage identification</li>
                            <li>Broker payout tracking vs revenue generated</li>
                        </ul>
                    </li>
                </ol>

                <h3>Module 3: Sales Performance Snapshot</h3>
                <ol>
                    <li><strong>KPIs tracked</strong></li>
                    <ul>
                        <li>Sales Conversion Rate (%) = (Converted Leads / Total Leads) × 100</li>
                        <li>Top 5 Performing Regions (by premium collected)</li>
                        <li>Top 5 Selling Products</li>
                        <li>Top 5 Agent Contributions</li>
                        <li>Best Performing Channel (e.g., Direct, Bancassurance, Broker, Online)</li>
                    </ul>
                    <li>
                        <strong>Insights Delivered:</strong>
                        <ul>
                            <li>Regional performance heatmap</li>
                            <li>Channel optimisation recommendations</li>
                            <li>Agent incentives performance tracking</li>
                        </ul>
                    </li>
                </ol>

                <h3>Module 4: Actuarial Risk & Pricing Insights</h3>
                <ol>
                    <li><strong>KPIs tracked</strong></li>
                    <ul>
                        <li>Loss Ratio (%) = (Incurred Claims / Earned Premium) × 100</li>
                        <li>Persistency Ratio (%) = (Renewed Policies / Renewable Policies) × 100</li>
                        <li>Policy Lapse Rate (%) = (Lapsed Policies / Total In-Force Policies) × 100</li>
                        <li>New vs Renewal Premium Split</li>
                        <li>Average Sum Insured per Policy/Product</li>
                        <li>Projected vs Actual Claims Trend</li>
                        <li>Risk-Adjusted Profitability Index (per product line)</li>
                    </ul>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <ul>
                        <li><strong>Data Integration:</strong> Unified datasets across departments using Policy Number +
                            Start Date as a common key</li>
                        <li><strong>KPI Calculations:</strong> Built modular SQL scripts and DAX measures to compute
                            department-specific KPIs</li>
                        <li><strong>Dashboard Development:</strong> Used Excel and/or Power BI to deliver interactive
                            and printable dashboards</li>
                        <li><strong>Data Dump:</strong> Provided detailed raw exports for backend teams in each
                            department</li>
                    </ul>
                </ol>

                <h3>Validation and Testing</h3>
                <ol>
                    <ul>
                        <li>Conducted User Acceptance Testing (UAT) with all HODs</li>
                        <li>Collected iterative feedback for enhancements</li>
                        <li>Ensured report alignment with each department’s strategic and operational KPIs</li>
                    </ul>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <strong>Business Impact</strong>
                    <ul>
                        <li>Enabled real-time, MTD business health monitoring across verticals</li>
                        <li>Supported data-driven discussions in daily/weekly leadership meetings</li>
                        <li>Improved cross-departmental awareness of performance drivers</li>
                        <li>Became a standard reference tool for MD reviews, strategic discussions, and forecasting</li>
                    </ul>
                </ol>
            </div>

            <div id="CRM_Dashboard" class="box">
                <h3><u>Company Project 5: CRM Dashboard – Customer Satisfaction & Service KPIs</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">
                    The project aimed to design and deliver an interactive CRM dashboard for Care Health Insurance
                    to
                    monitor customer satisfaction (NPS), service performance, and retention trends. The goal was to
                    provide
                    leadership and operational teams with actionable insights to improve customer experience, reduce
                    churn,
                    and enhance service efficiency.
                </p>
                <ol class="hanging-indent">
                    <li><strong>Customer Insights:</strong> Analysed churn drivers, NPS trends, and service
                        performance gaps
                        impacting customer satisfaction and renewals.</li>
                    <li><strong>Business Intelligence Model:</strong> Developed an interactive dashboard in Power
                        BI/Qlik Sense
                        to visualise NPS distribution, service KPIs, churn vs. renewals, and agent-level sales
                        conversion.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection and Processing</strong>
                        <ul>
                            <li>Extracted customer, claims, and service records using SQL, Alteryx, and Qlik ETL.
                            </li>
                            <li>Integrated multiple sources into a unified data model covering:
                                <ol>Customer Feedback: Net Promoter Score (Promoters, Passives, Detractors).</ol>
                                <ol>Service KPIs: Resolution Time, Escalation Rate, First Contact Resolution.</ol>
                                <ol>Renewal vs. Churn: Active vs. lapsed customers, policy renewals, retention
                                    rates.</ol>
                                <ol>Sales Engagement: Lead response time, conversion rate, and upsell/cross-sell
                                    metrics.</ol>
                            </li>
                        </ul>
                    </li>
                    <li><strong>Data Preprocessing and Feature Engineering</strong>
                        <ul>
                            <li>Derived key metrics for service and retention analysis:
                                <ol>Churn Rate = (Cancelled Policies / Total Active Policies).</ol>
                                <ol>Customer Retention % = (Renewed Policies / Expiring Policies).</ol>
                                <ol>Average Resolution Time = (Total Time to Resolve Queries / Number of Queries).
                                </ol>
                                <ol>Escalation % = (Escalated Cases / Total Cases).</ol>
                            </li>
                            <li>Implemented automated refreshes and applied role-based access for secure stakeholder
                                usage.</li>
                        </ul>
                    </li>

                    <li><strong>Dashboard Development and Insights</strong>
                        <ul>
                            <li>Customer Satisfaction:
                                <ol>NPS distribution (Promoters, Passives, Detractors).</ol>
                                <ol>NPS trend line over time by region and policy type.</ol>
                            </li>
                            <li>Service KPIs:
                                <ol>Resolution time, escalation trends, and first contact resolution rates.</ol>
                                <ol>Complaint categorisation and backlog monitoring.</ol>
                            </li>
                            <li>Retention Analysis:
                                <ol>Renewal vs churn rates by product and region.</ol>
                                <ol>Customer segment-level retention insights.</ol>
                            </li>
                            <li>Sales & Engagement:
                                <ol>Lead response time by channel.</ol>
                                <ol>Conversion rate and cross-sell/upsell performance.</ol>
                            </li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Customer Experience Enhancement</strong>
                        <ul>
                            <li>Revealed churn drivers: 40% premium hikes, 30% poor renewal reminders, 20%
                                unresolved claims.</li>
                            <li>Introduced automated renewal reminders and improved claim communication processes.
                            </li>
                            <li>Led to a <strong>15–20% increase in customer retention</strong> within three months.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Operational Efficiency Gains</strong>
                        <ul>
                            <li>Improved visibility into service KPIs and escalations, enabling faster issue
                                resolution.</li>
                            <li>Empowered leadership with real-time actionable insights for CRM strategy and
                                customer engagement.</li>
                            <li>Standardised KPIs across departments, reducing ad-hoc reporting by 25%.</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <div id="Repeated_Claims_Detection" class="box">
                <h3><u>Company Project 6: Repeated Claims Detection – Fraud Prevention</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">
                    The project aimed to detect and prevent fraudulent health insurance claims by identifying repeated
                    claims submitted by the same insured member on the same date of admission. Using Snowflake as the
                    data source,
                    the initiative focused on building automated checks, integrating outputs with medical and payments
                    teams,
                    and developing a predictive model to proactively flag high-risk claims.
                </p>
                <ol class="hanging-indent">
                    <li><strong>Fraud Identification:</strong> Identified duplicate/repeated claims at source (MTD
                        claims data from Snowflake).</li>
                    <li><strong>Process Integration:</strong> Shared flagged cases with Medical and Payment teams to
                        prevent approval of fraudulent claims.</li>
                    <li><strong>Predictive Model:</strong> Built a fraud detection model to proactively identify
                        suspicious claims for investigation.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection and Processing</strong>
                        <ul>
                            <li>Extracted Month-to-Date (MTD) claims data from Snowflake covering claim details, insured
                                member IDs, admission dates, and claim statuses.</li>
                            <li>Defined claim status priority order: <strong>Paid → Outstanding →
                                    Cancelled/Rejected</strong> to ensure correct handling of duplicates.</li>
                            <li>Created a composite key = <strong>Insured Member ID + Date of Admission</strong> to
                                detect repeated claims.</li>
                        </ul>
                    </li>

                    <li><strong>Duplicate Detection Logic</strong>
                        <ul>
                            <li>Used SQL queries in Qlik to identify multiple claims with the same insured member
                                and admission date.</li>
                            <li>Applied prioritisation logic to retain only the valid claim record (Paid/Outstanding)
                                and flag duplicates (Cancelled/Rejected).</li>
                            <li>Generated exception reports highlighting repeated claims for daily review by medical and
                                payment teams.</li>
                        </ul>
                    </li>

                    <li><strong>Predictive Model Development</strong>
                        <ul>
                            <li>Performed Exploratory Data Analysis (EDA) using Python and Pandas to identify fraud
                                indicators such as:
                                <ol>High frequency of claims by the same member in short time windows.</ol>
                                <ol>Unusual admission-discharge patterns or same-day readmissions.</ol>
                                <ol>High-value claims repeated with minor variations.</ol>
                            </li>
                            <li>Engineered fraud-related features:
                                <ol>Duplicate_Claim_Flag, Frequency_in_30Days, Avg_Claim_Amount_by_Member,
                                    Hospital_Risk_Score.</ol>
                            </li>
                            <li>Built classification models (Logistic Regression, Random Forest, XGBoost) to predict
                                probability of fraud.</li>
                            <li>Selected Random Forest based on performance metrics (AUC = 0.87, Precision = 82%, Recall
                                = 78%).</li>
                        </ul>
                    </li>

                    <li><strong>Process Implementation</strong>
                        <ul>
                            <li>Deployed SQL checks in Snowflake for automated duplicate detection.</li>
                            <li>Integrated predictive model outputs into dashboard view for Medical and Payments teams.
                            </li>
                            <li>Created a daily fraud risk report with high-probability cases flagged for manual
                                investigation.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Fraud Prevention</strong>
                        <ul>
                            <li>Prevented repeated claims from being processed by creating a unique composite key
                                system.</li>
                            <li>Blocked suspicious claims at source, saving significant costs in potential fraudulent
                                payouts.</li>
                            <li>Improved regulatory compliance by maintaining accurate and validated claim records.</li>
                        </ul>
                    </li>

                    <li><strong>Business Impact</strong>
                        <ul>
                            <li>Medical and Payments teams reduced manual duplicate checks by over 60%.</li>
                            <li>The predictive fraud model helped proactively flag <strong>high-risk claims with ~80%
                                    accuracy</strong>.</li>
                            <li>Strengthened client trust by ensuring tighter fraud controls and improved audit
                                readiness.</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <div id="Sales_Incentive_Targets" class="box">
                <h3><u>Company Project 7: Sales Incentive Dashboard</u></h3>

                <h3>Objective</h3>
                <p class="justified-text">
                    Build an automated, role-aware incentive dashboard for Frontline Sales Officers (FSOs),
                    Area Managers, and Zonal Managers, where <strong>targets were fixed for every month and
                        quarter</strong>.
                    The goal was to give real-time visibility of <em>Target vs Actual</em>, remove payout errors,
                    and enable mid-cycle course correction so teams could consistently exceed targets.
                </p>
                <ol class="hanging-indent">
                    <li><strong>Transparency:</strong> Show each role exactly where they stood against
                        fixed monthly/quarterly targets, updated daily.</li>
                    <li><strong>Accuracy:</strong> Automate tiered payout logic to eliminate manual errors and disputes.
                    </li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection & Processing</strong>
                        <ul>
                            <li>Integrated daily sales and renewals into Snowflake via SQL/Alteryx; aligned with HR
                                payroll data.</li>
                            <li>Loaded <strong>fixed targets</strong> (monthly/quarterly) by role: FSO, Area, Zone.</li>
                        </ul>
                    </li>

                    <li><strong>Rules & Feature Engineering</strong>
                        <ul>
                            <li>Defined KPIs and payout tiers per role:
                                <ol>FSO: Policies sold, Conversion rate, Cross-sell/Upsell.</ol>
                                <ol>Area Manager: Aggregated FSO performance vs area targets.</ol>
                                <ol>Zonal Manager: Regional revenue growth, Retention KPIs.</ol>
                            </li>
                            <li>Created DAX measures for <em>Target vs Actual</em>, % Achievement, Tier
                                (Bronze/Silver/Gold).</li>
                        </ul>
                    </li>

                    <li><strong>Dashboard Development</strong>
                        <ul>
                            <li><strong>Target vs Actual Gauges</strong> for Month and Quarter at FSO → Area → Zone
                                levels.</li>
                            <li><strong>Leaderboards</strong> to rank FSOs and teams; drilldowns from Zone → Area → FSO.
                            </li>
                            <li><strong>Role-based access</strong>: FSOs saw self; managers saw aggregates; HR saw
                                payout validation.</li>
                            <li>Automated refresh; PDF snapshots for monthly/quarterly reviews.</li>
                        </ul>
                    </li>

                    <li><strong>Adoption & Governance</strong>
                        <ul>
                            <li>Ran training with Sales & HR; published usage SOP; logged payout audit trails.</li>
                            <li>Embedded dashboard in weekly huddles and month-end target war-rooms.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Operational Impact</strong>
                        <ul>
                            <li>100% payout accuracy; payroll preparation time reduced from weeks to days.</li>
                            <li>Managers coached mid-cycle using live <em>% Achievement</em> and pipeline visibility.
                            </li>
                        </ul>
                    </li>
                    <li><strong>Business Outcome</strong>
                        <ul>
                            <li>With fixed targets set every month and quarter, teams consistently
                                <strong>crossed targets by ~20%</strong> in the next quarter and maintained
                                double-digit uplift over six months.
                            </li>
                            <li>Drivers: real-time target tracking for FSOs, focused coaching by Area/Zonal leaders,
                                and clear tiered incentives tied to strategic KPIs (renewals, cross-sell).</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <div id="Snowflake_to_Star_Schema" class="box">
                <h3><u>Company Project 8: Data Warehouse Optimisation — Snowflake to Star Schema Migration</u></h3>

                <h3>Objective</h3>
                <p class="justified-text">
                    Redesign and optimise the enterprise data warehouse by migrating from a
                    <strong>Snowflake Schema</strong> to a <strong>Star Schema</strong> to simplify joins,
                    improve dashboard refresh performance, and <strong>reduce ETL runtime from 8 hours to under 5
                        hours</strong>.
                    The initiative ensured <strong>95%+ data accuracy on T-1 logic (Today – 1)</strong>
                    while improving cross-pipeline consistency and report reliability.
                </p>

                <ol class="hanging-indent">
                    <li><strong>Transparency:</strong> Provide a unified, conformed data model across Claims, Sales,
                        Finance, and Product departments.</li>
                    <li><strong>Performance:</strong> Optimise ETL and query logic to accelerate refresh cycles, reduce
                        compute costs, and enhance stability.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection & Processing</strong>
                        <ul>
                            <li>Profiled 40+ fact and dimension tables using SQL and Qlik ETL logs to identify redundant
                                hierarchies and joins.</li>
                            <li>Extracted and staged data via <strong>SQL, Alteryx, and Qlik ETL</strong> workflows with
                                incremental loads using watermark control tables.</li>
                            <li>Built modular ETL pipelines (staging → dimension → fact → publish) to enable parallel
                                processing and improve orchestration efficiency.</li>
                            <li>Collaborated with IT to create a <strong>replica environment</strong> for testing and
                                validation before production deployment.</li>
                        </ul>
                    </li>

                    <li><strong>Rules & Feature Engineering</strong>
                        <ul>
                            <li>Defined clear fact-level grain (e.g., one row per claim transaction or policy sale).
                            </li>
                            <li>Collapsed snowflaked dimensions into <strong>denormalised conformed dimensions</strong>
                                such as <em>dim_customer</em> and <em>dim_product</em> with surrogate keys.</li>
                            <li>Implemented <strong>SCD Type 2</strong> for historical attributes (region, segment) and
                                <strong>Type 1</strong> for static fields (names, IDs).
                            </li>
                            <li>Shifted some <strong>front-end calculations</strong> (KPIs, derived measures)
                                into <strong>backend Python and Qlik scripts</strong> for better performance and reduced
                                DAX load time.</li>
                            <li>Reviewed and commented out <strong>unused or redundant dashboards</strong> to reduce
                                refresh overhead and focus on high-value reports.</li>
                        </ul>
                    </li>

                    <li><strong>ETL Optimisation & File-Based Trigger Mechanism</strong>
                        <p class="justified-text">
                            In my previous organisation, we were dealing with multiple ETL jobs pulling data from
                            various
                            sources, often on overlapping schedules. To ensure data accuracy and consistency across
                            pipelines, I implemented a <strong>file-based trigger mechanism</strong>.
                        </p>
                        <ul>
                            <li><strong>What I Did:</strong> Introduced a “file variable” check in the ETL workflow.
                                Each job only started once a
                                specific control file was updated, ensuring data freshness and dependency integrity.
                            </li>
                            <li><strong>Why It Worked:</strong>
                                <ol>
                                    <li>Prevented race conditions where one job could start before upstream data was
                                        ready.</li>
                                    <li>Ensured consistent data across multi-source pipelines (claims, sales, HR,
                                        finance).</li>
                                    <li>Acted as a lightweight but reliable <strong>validation gate</strong> before
                                        transformation began.</li>
                                </ol>
                            </li>
                        </ul>
                    </li>

                    <li><strong>Orchestration & Testing</strong>
                        <ul>
                            <li>Replaced nested SQL joins with <strong>materialised staging tables</strong> to reduce
                                I/O load and runtime.</li>
                            <li>Implemented <strong>parallel dimension builds</strong> via Qlik QMC and Alteryx task
                                scheduling.</li>
                            <li>Introduced <strong>T-1 logic (Today – 1)</strong> to ensure all daily reports used
                                validated, complete data.</li>
                            <li>Analysed ETL logs, identified bottlenecks, and re-sequenced heavy joins for faster
                                throughput.</li>
                            <li>Cross-checked report numbers and KPIs between <strong>Production vs Replica</strong>
                                environments
                                to verify consistency and accuracy.</li>
                        </ul>
                    </li>

                    <li><strong>Dashboard & BI Integration</strong>
                        <ul>
                            <li>Re-mapped Power BI and Qlik dashboards to the new <strong>Star Schema views</strong>,
                                cutting visual load time from 45 min to 15 min.</li>
                            <li>Rebuilt DAX and Qlik expressions to leverage backend-calculated KPIs.</li>
                            <li>Used Snowflake and Alteryx outputs to drive refreshed Power BI datasets and
                                auto-distributed snapshots.</li>
                        </ul>
                    </li>

                    <li><strong>Adoption & Governance</strong>
                        <ul>
                            <li>Documented schema mappings, ETL lineage, and KPI logic for version control.</li>
                            <li>Trained BI and data engineering teams on the new data model, dependencies, and load
                                monitoring.</li>
                            <li>Introduced <strong>automated data validation scripts</strong> (row count, totals, null
                                checks) with run-time logs.</li>
                            <li>Set up audit dashboards to monitor ETL status, refresh duration, and data freshness
                                KPIs.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Operational Impact</strong>
                        <ul>
                            <li>ETL runtime reduced from <strong>8 hours → 4 hours 45 minutes</strong> (~40% faster).
                            </li>
                            <li>All published reports achieved <strong>95%+ accuracy</strong> on <em>T-1 logic</em>.
                            </li>
                            <li>Dashboard refresh times improved by 66% (45 min → 15 min).</li>
                            <li>Log file analysis enabled early detection and prevention of pipeline failures.</li>
                            <li>Manual restarts eliminated through fully automated job sequencing.</li>
                        </ul>
                    </li>

                    <li><strong>Business Outcome</strong>
                        <ul>
                            <li>Leadership teams gained <strong>real-time KPI visibility by 6 AM</strong>, improving
                                decision-making agility.</li>
                            <li>Compute cost reduced via schema simplification and incremental loading.</li>
                            <li>Enabled consistent data across departments, strengthening collaboration between IT, BI,
                                and Operations.</li>
                            <li>High scalability achieved for future integrations and additional product lines.</li>
                            <li><strong>Drivers:</strong> Schema simplification, file-based trigger automation, backend
                                KPI computation, and proactive QA validation.</li>
                        </ul>
                    </li>
            </div>

            <div id="Automated_Extraction" class="box">
                <h3><u>Self Project 1: Automated Data Pipeline for Kaggle Datasets - Python</u>
                </h3>

                <h3>Objective</h3>
                <p class="justified-text">
                    This project focuses on building a modular and reusable Python pipeline that
                    automates the ingestion
                    of datasets from Kaggle, performs exploratory data analysis (EDA), and
                    exports
                    cleaned datasets for
                    use in BI tools (e.g., Power BI, Tableau) or machine learning models. The
                    goal
                    was to streamline
                    repetitive steps, promote reproducibility, and apply practical data
                    engineering
                    principles to
                    real-world datasets.
                </p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Kaggle Extractor Module</strong>
                        <ul>
                            <li>A custom Python module <code>Kaggle_Extractor.py</code> was
                                created
                                to handle automated
                                data ingestion.</li>
                            <li><strong>Key Features:</strong></li>
                            <ul>
                                <li><strong>Automated Data Downloading:</strong> Uses Kaggle API
                                    to
                                    download ZIP files
                                    directly from a given dataset path (e.g.,
                                    <code>"sakshigoyal7/credit-card-customers"</code>).
                                </li>
                                <li><strong>ZIP Extraction & File Management:</strong>
                                    Automatically
                                    extracts ZIP files
                                    and stores them in project-specific folders.</li>
                                <li><strong>Dynamic DataFrame Loading:</strong> Loads all
                                    extracted
                                    CSVs into pandas
                                    DataFrames without manual naming or inspection.</li>
                            </ul>
                            <p>This acts as the data ingestion layer, a key component in a
                                scalable
                                data engineering
                                workflow.</p>
                        </ul>
                    </li>

                    <li><strong>EDA and Transformation Script</strong>
                        <ul>
                            <li>The <code>EDA_Script.py</code> is built to run on any dataset
                                retrieved via the
                                extractor.</li>
                            <li><strong>Key EDA Steps:</strong></li>
                            <ul>
                                <li>Displays shape, column names, data types, descriptive
                                    statistics, and missing values
                                </li>
                                <li>Visualises distributions using histograms and identifies
                                    correlations with heatmaps
                                </li>
                                <li>Drops rows with missing values and isolates numeric columns
                                    for
                                    modeling</li>
                                <li>Auto-selects a numeric target column (last one by default)
                                    for
                                    regression modeling
                                </li>
                            </ul>
                            <li>Applies standardisation via <code>StandardScaler</code> to
                                prepare
                                data for ML workflows
                            </li>
                            <li>Saves the cleaned dataset with a dynamic filename based on the
                                dataset, file name, and
                                timestamp</li>
                        </ul>
                    </li>

                    <li><strong>Prototype Machine Learning (Optional)</strong>
                        <ul>
                            <li>Fits a basic Linear Regression model if a valid numeric target
                                is
                                found</li>
                            <li>Includes data splitting, scaling, model training, prediction,
                                and
                                MSE evaluation</li>
                            <li>Designed to demonstrate plug-and-play modeling on newly ingested
                                data</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ul>
                    <li>Successfully built a modular and automated data pipeline using Python
                    </li>
                    <li>Eliminated repetitive manual tasks like dataset downloads, inspection,
                        and
                        setup</li>
                    <li>Enabled quick turnaround from raw data to BI-ready files</li>
                    <li>Established a strong foundation for future integration with dashboards
                        or
                        more advanced modeling
                    </li>
                </ul>

                <h3>Next Steps</h3>
                <ul>
                    <li>Support for categorical encoding and missing value imputation</li>
                    <li>Integration with Streamlit or Dash for real-time dashboarding</li>
                    <li>Enhanced model selection and evaluation via automated reporting</li>
                </ul>

                <h3>Relevant Files</h3>
                <ol>
                    <li><strong>EDA Script</strong>
                        <p class="justified-text">
                            <a href="Documents/Projects/Kaggle/EDA_Script.py" target="_blank">Click
                                to Preview</a>
                        </p>
                    </li>
                    <li><strong>Kaggle Extractor</strong>
                        <p class="justified-text">
                            <a href="Documents/Projects/Kaggle/Kaggle_Extractor.py" target="_blank">Click to
                                Preview</a>
                        </p>
                    </li>
                </ol>
            </div>

            <div id="Python_ETL_PBI_Visualisation_ML" class="box">
                <h3><u>Self Project 2: Monthly Beverage Sales Dashboard – Power BI + Python</u>
                </h3>

                <h3>Objective</h3>
                <p class="justified-text">
                    This project aimed to create an end-to-end sales dashboard solution using
                    Power
                    BI and Python,
                    driven by a
                    Kaggle dataset on synthetic beverage sales. The dashboard provides a rich,
                    interactive analysis of
                    sales KPIs,
                    customer behavior, product trends, and regional insights. It also integrates
                    a
                    reusable Python data
                    pipeline
                    to automate data ingestion, cleaning, and transformation.
                </p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Kaggle Extractor Module</strong>
                        <ul>
                            <li>Developed a custom Python module
                                <code>Kaggle_Extractor.py</code> to
                                automate data
                                ingestion.
                            </li>
                            <li><strong>Key Features:</strong></li>
                            <ul>
                                <li><strong>Automated Data Downloading:</strong> Uses the Kaggle
                                    API
                                    to download ZIP
                                    datasets via
                                    dataset path (e.g.,
                                    <code>"sebastianwillmann/beverage-sales"</code>).
                                </li>
                                <li><strong>ZIP Extraction & File Management:</strong> Extracts
                                    and
                                    stores datasets in
                                    structured
                                    folders.</li>
                                <li><strong>Dynamic DataFrame Loading:</strong> Loads CSV files
                                    into
                                    pandas DataFrames
                                    automatically.</li>
                            </ul>
                            <p>This serves as the data ingestion layer of the project and is
                                easily
                                reusable across
                                different datasets.</p>
                        </ul>
                    </li>

                    <li><strong>Data Cleaning and Preparation (Python)</strong>
                        <ul>
                            <li>CSV merging, null handling, and feature engineering in
                                <code>EDA_Script.py</code>.
                            </li>
                            <li><strong>Engineered Features:</strong></li>
                            <ul>
                                <li>Date columns: <code>Year</code>, <code>Month</code>,
                                    <code>MonthYear</code>
                                </li>
                                <li><code>Is_Weekend</code> flag based on order day</li>
                            </ul>
                            <li><strong>Data Volume Handling:</strong> Reduced original dataset
                                from
                                <b>8.9 million rows
                                    to 1 million</b>
                                to improve performance and allow sanity checks within system
                                memory
                                limits.
                            </li>
                            <li>Exports cleaned CSV with a timestamped filename for Power BI use
                            </li>
                        </ul>
                    </li>

                    <li><strong>Power BI Modeling and DAX</strong>
                        <ul>
                            <li>Loaded cleaned data into Power BI using Power Query</li>
                            <li>Created calculated columns and DAX measures for KPIs and trends
                            </li>
                            <li><strong>Core DAX KPIs:</strong></li>
                            <ul>
                                <li><code>Total_Sales</code>, <code>Total_Customers</code>,
                                    <code>Total_Orders</code>,
                                    <code>Avg_Order_Value</code>
                                </li>
                                <li>Advanced: <code>Customer_Lifetime_Value</code>,
                                    <code>Customer_Recency</code>,
                                    <code>Product_Popularity</code>,
                                    <code>Revenue_per_Unit</code>,
                                    <code>YoY_Growth</code>
                                </li>
                            </ul>
                        </ul>
                    </li>

                    <li><strong>Dashboard Pages and Visuals</strong>
                        <ul>
                            <li><b>Overview:</b> KPI Cards, Top Product/Customer/Region</li>
                            <li><b>Top 10s:</b> Products, Customers, Categories, Regions</li>
                            <li><b>Customer Trends:</b> Lifetime Value, Frequency Trend</li>
                            <li><b>RFM & Returns:</b> Customer Segments, Returns, Recency
                                Distribution</li>
                            <li><b>Product Analysis:</b> Popularity, Revenue per Unit, Category
                                Bar
                                Chart</li>
                            <li><b>Geospatial:</b> Filled and bubble maps of region-wise sales
                            </li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ul>
                    <li>Automated pipeline from Kaggle to Power BI using Python and DAX</li>
                    <li>Improved reproducibility and reduced development time</li>
                    <li>Enabled deeper business analysis with custom KPIs and segmentation</li>
                    <li>Scalable design, pipeline can be adapted for other datasets with minimal
                        effort</li>
                </ul>

                <h3>Next Steps</h3>
                <ul>
                    <li>Add forecasting visuals using Power BI’s analytics pane</li>
                    <li>Integrate data refresh automation via Power BI Service</li>
                    <li>Introduce custom themes and trendlines for better UX</li>
                    <li>Parameterise top-N filters and drill-through charts for advanced
                        interactivity</li>
                </ul>

                <h3>Opportunities for Predictive Insights</h3>
                <ul>
                    <li><b>Customer Churn Prediction:</b> Identify customers with declining
                        engagement</li>
                    <li><b>RFM Scoring:</b> Implement numerical scoring for Recency, Frequency,
                        and
                        Monetary value</li>
                    <li><b>Basket Analysis:</b> Discover frequently bought product combinations
                    </li>
                    <li><b>Discount Elasticity:</b> Analyse how discount levels affect product
                        sales
                        and revenue</li>
                </ul>

                <p class="justified-text">
                    All data was extracted, processed, and cleaned using my custom-built
                    <code>Kaggle_Extractor.py</code> module, ensuring a repeatable and modular
                    pipeline across multiple
                    Kaggle datasets.
                </p>

                <h3>Relevant Files</h3>

                <ol>
                    <li><strong>.pbix Dashboard</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Monthly Beverage_Sales_Dashboard/Beverages_Sales.pbix"
                                target="_blank">Click to Preview</a></p>
                    </li>
                    <li><strong>Readme</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Monthly Beverage_Sales_Dashboard/PowerBI_Sales_Dashboard_Report.docx"
                                target="_blank">Click to Preview</a></p>
                    </li>
            </div>

            <div id="Predictive_Model" class="box">
                <h3><u>Project 1: Predicting London Bike Usage Using Machine Learning -
                        Python</u>
                </h3>

                <h3>Objective</h3>
                <p class="justified-text">
                    This project aimed to address operational challenges in London's
                    bike-sharing
                    scheme by analysing
                    and forecasting bike usage patterns. By leveraging machine learning models
                    and
                    combining trip data
                    with weather insights, the goal was to optimise bike availability and
                    improve
                    service distribution
                    across the city.
                </p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Collection and Processing</strong>
                        <ul>
                            <li><strong>Urban Mobility Data:</strong> Sourced from Transport for
                                London (TfL), including
                                trip start/end times, durations, and station information for
                                June
                                2023.</li>
                            <li><strong>Weather Data:</strong> Collected from Visual Crossing
                                for
                                July–September 2023,
                                incorporating temperature, humidity, wind speed, and
                                precipitation.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Data Preprocessing and Feature Engineering</strong>
                        <ul>
                            <li>Removed outliers and filtered trips shorter than 1 minute or
                                longer
                                than 24 hours.</li>
                            <li>Engineered new features: Day of the week, Hour of day / time
                                bins,
                                Trip duration and
                                routes, Weather-related columns merged by date
                            </li>
                            <li>Used <code>MinMaxScaler</code> to normalise numerical features
                                before training.</li>
                        </ul>
                    </li>

                    <li><strong>Machine Learning Models</strong>
                        <ul>
                            <li>Tested multiple algorithms: Linear Regression, Decision Trees,
                                Random Forest, Neural
                                Networks, Support Vector Machines (SVM), XGBoost</li>
                            <li>Evaluation metrics: Mean Squared Error (MSE), Mean Absolute
                                Error
                                (MAE), R-Squared (R²)
                            </li>
                    </li>
                    <li><strong>Outcome:</strong> Linear Regression performed best for
                        predicting
                        bike usage based on
                        temperature, followed by Decision Trees and Random Forest.</li>
                    </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Urban Mobility Insights</strong>
                        <ul>
                            <li>Peak usage occurred in the afternoon, with the highest rental day being June
                                21, 2023.
                            </li>
                            <li>Hyde Park Corner emerged as the most active station, reflecting high central
                                London
                                traffic.</li>
                            <li>Weekday afternoons, especially Thursdays and Fridays, saw the highest rental
                                volumes, suggesting a post-work travel pattern.</li>
                        </ul>
                    </li>

                    <li><strong>Predictive Model Findings</strong>
                        <ul>
                            <li>Temperature was the strongest predictor of ride volume, with Linear
                                Regression giving
                                the most consistent performance.</li>
                            <li>Future improvements:
                                <ul>
                                    <li>Introduce non-linear models (e.g., SVM with RBF kernels)</li>
                                    <li>Include additional features like humidity, event data, and station
                                        capacity</li>
                                </ul>
                            </li>
                            <li><strong>Operational Recommendation:</strong> Use weather forecasts to plan
                                bike
                                distribution—stock more bikes at key stations on warm days.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Relevant Files</h3>

                <ol>
                    <li><strong>Questionnaire</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Predicting_London_Bike_Usage/MATH6185 Case Study 1.pdf"
                                target="_blank">Click to Preview</a></p>
                    </li>
                    <li><strong>Report</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Predicting_London_Bike_Usage/Report-28965736.pdf"
                                target="_blank">Click to Preview</a></p>
                    </li>
                    <li><strong>Python File for Exploratory Data Analytics</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Predicting_London_Bike_Usage/Ques1-28965736.py"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
                    <li><strong>Python File for Machine Learning</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Predicting_London_Bike_Usage/Ques2-28965736.py"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
            </div>
            <!-- </div>
  </div> -->
            <!-- Project 2: Examining the Role of Data Analytics in Banking Decision-Making -->
            <!-- <div class="dropdown" style="position: relative;">
      <button class="dropdown-btn">Examining the Role of Data Analytics in Banking Decision-Making</button>
      <div class="dropdown-content"> -->
            <div id="Power_BI" class="box">
                <h3><u>Project 2: Examining the Role of Data Analytics in Banking Decision-Making - Power
                        BI</u>
                </h3>
                <h3>Objective</h3>
                <p class="justified-text">The report's primary objective is to examine how data analytics
                    (DA)
                    impacts
                    decision-making
                    processes within financial institutions, specifically in a banking context. Key aims
                    include:
                </p>
                <ol class="hanging-indent">
                    <li><strong>Evaluating the Benefits of Data Analytics:</strong>Enhancing
                        decision-making,
                        customer
                        service, risk management, and competitive advantages in the banking industry.</li>
                    <li><strong>Identifying Challenges in DA Implementation:</strong>Highlighting barriers
                        such as
                        technological, social, and cognitive challenges.</li>
                    <li><strong>Assessing Data Quality:</strong>Examining data quality’s importance for
                        effective
                        decision-making and strategies to improve it.</li>
                    <li><strong>Providing Visualisation and Insights:</strong>Using data visualisation
                        techniques to
                        analyse sales, profit trends, and product performance across regions and customer
                        segments.
                    </li>
                </ol>
                <p class="justified-text">The overarching objective is to analyse DA's role in improving
                    organisational
                    decision-making and
                    operational efficiency within financial institutions.</p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Theoretical Framework and Challenges (Part A)</strong>
                        <ul>
                            <li>Literature Review: Theories such as PSI, dual theory, and critical
                                skepticism
                                explore DA
                                adoption barriers.</li>
                            <li>Data Quality Assessment: Evaluates metrics like accuracy, consistency, and
                                reliability,
                                critical in regulated environments.</li>
                            <li>Strategies for Improving Data Quality: Recommends data governance
                                frameworks, data
                                cleaning tools, centralised repositories, and training.</li>
                        </ul>
                    </li>

                    <li><strong>Data Visualisation and Analysis (Part B)</strong>
                        <ul>
                            <li>Visualisation Tools: Power BI charts, such as stacked columns, pie charts,
                                and
                                clustered
                                columns.</li>
                            <li>Sales and Profit Analysis: Examines product performance across regions and
                                time
                                trends
                                (2014-2017).</li>
                            <li>Customer Segment Analysis: Analyses sales and profit by customer segment
                                (consumer,
                                corporate, home office).</li>
                            <li>Return Analysis: Identifies regions and categories with high return rates,
                                suggesting
                                quality control improvements.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Key Findings</strong>
                        <ul>
                            <li>Data Analytics Benefits: Enhances decision-making accuracy, segmentation,
                                and risk
                                management.</li>
                            <li>Challenges in DA Implementation: Barriers include integration issues, staff
                                resistance,
                                and data quality problems.</li>
                            <li>Data Quality Importance: Essential for accurate DA outcomes, regulatory
                                compliance,
                                and
                                customer satisfaction.</li>
                            <li>Insights from Data Visualisation:
                                <ul>
                                    <li>East and West regions show highest sales; Central and South lag
                                        behind.</li>
                                    <li>Office supplies have low profit margins despite high sales.</li>
                                    <li>High sales regions correlate with high return rates, especially for
                                        tech
                                        products.</li>
                                </ul>
                            </li>
                        </ul>
                    </li>

                    <li><strong>Recommendations</strong>
                        <ul>
                            <li>Data Quality Improvements: Implement data governance, audits, and
                                centralised
                                repositories.</li>
                            <li>Targeted Regional Strategies: Improve sales in underperforming regions
                                through
                                localised
                                marketing.</li>
                            <li>Product Portfolio Optimisation: Replace low-performing items with
                                high-demand
                                products.
                            </li>
                            <li>Enhanced Customer Support and Return Policies: Improve customer support and
                                manage
                                return volumes with stricter policies.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Relevant Files</h3>
                <ol>
                    <li><strong>Questionnaire</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/PBI/MANG6526%20-%20Group%20Coursework%202024.pdf"
                                target="_blank">Click to Preview</a></p>
                    </li>
                    <li><strong>Report</strong>
                        <p class="justified-text"><a href="Documents/Projects/PBI/Power%20BI%20-%2028965736.pdf"
                                target="_blank">Click to
                                Preview</a></p>
                    </li>
                    <li><strong>Power BI File</strong>
                        <p class="justified-text"><a href="Documents/Projects/PBI/Group%2033.pbix" target="_blank">Click
                                to Preview</a></p>

                    </li>
                    <li><strong>Data File</strong>
                        <p class="justified-text"><a href="Documents/Projects/PBI/Alpha%20Stores.xlsx"
                                target="_blank">Click
                                to Preview</a></p>
            </div>
            <!-- </div>
  </div> -->
            <!-- Project 3: SQL Coursework Report on Healthcare Data Analytics -->
            <!-- <div class="dropdown" style="position: relative;">
                            <button class="dropdown-btn">SQL Coursework Report on Healthcare Data Analytics</button>
                            <div class="dropdown-content"> -->
            <div id="project3" class="box">
                <h3><u>Project 3: NHS Staff Scheduling</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The report aims to solve a series of SQL-based data analytics
                    tasks within
                    a
                    healthcare context, focusing on various aspects of staff management and operations.
                    Key objectives include:</p>
                <ol class="hanging-indent">
                    <li><strong>Calculating Work Hours and Staff Allocation:</strong>Determine the
                        number of hours worked by specific staff members and calculate staffing
                        requirements across departments and shifts.</li>
                    <li><strong>Identifying Employee Details:</strong>Use SQL queries to retrieve
                        detailed information about staff, such as birthdates and assigned departments.
                    </li>
                    <li><strong>Analysing Staff Distribution by Specialty:</strong>Examine staffing
                        needs across hospital specialties, shifts, and departments.</li>
                    <li><strong>Determining Staff Costs by Specialty:</strong>Calculate the financial
                        implications of staffing across various hospital specialties within a defined
                        timeframe.</li>
                </ol>
                <p class="justified-text">Overall, the objective is to apply SQL data manipulation
                    techniques to
                    extract
                    actionable insights from hospital data for resource optimisation, staffing analysis,
                    and cost management.</p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Retrieval and Filtering</strong>
                        <ul>
                            <li>Staff Hours Calculation: A query multiplies unique shift counts by 8
                                hours to determine total hours worked by a specific staff member.</li>
                            <li>Birth Year Filter: Filtering staff born in a particular year, with
                                results ordered from oldest to youngest.</li>
                            <li>Department-Specific Allocation: Joins and filters are used to identify
                                staff working in specific wards (e.g., Neurology) on certain dates.</li>
                        </ul>
                    </li>

                    <li><strong>Staffing Patterns and Allocation Analysis</strong>
                        <ul>
                            <li>Shift Matching for Security Review: Queries identify staff who worked
                                across multiple shifts in General Wards G1 and G2.</li>
                            <li>Shift Count by Staff Type: Aggregation functions calculate the number of
                                staff per shift type in the Emergency department, grouped by role.</li>
                        </ul>
                    </li>

                    <li><strong>Timeframe-Based Analysis</strong>
                        <ul>
                            <li>Monthly Work Hours Summation: Queries aggregate hours worked across
                                several months, grouped by role and month.</li>
                            <li>Cost Calculation for Each Specialty: Per-shift costs are calculated
                                using annual salaries, divided over 230 shifts.</li>
                        </ul>
                    </li>

                    <li><strong>Data Aggregation and Join Operations</strong>
                        <ul>
                            <li>Uses SQL joins to combine data from `people`, `allocation`, `band`, and
                                `ward` tables.</li>
                            <li>Aggregation functions like `COUNT`, `SUM`, and `GROUP BY` summarise data
                                by staff roles, specialties, and dates.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Staff Workload Analysis</strong>
                        <ul>
                            <li>Identifies total hours worked by individuals and departments, aiding
                                resource allocation to high-demand areas.</li>
                        </ul>
                    </li>

                    <li><strong>Targeted Staff Information Retrieval</strong>
                        <ul>
                            <li>SQL queries retrieve employee-specific information, aiding HR in
                                workforce planning and demographic analysis.</li>
                        </ul>
                    </li>

                    <li><strong>Insights into Staffing Distribution</strong>
                        <ul>
                            <li>Staffing data by shift and specialty reveals staff distribution across
                                departments, helping adjust levels to meet patient needs.</li>
                        </ul>
                    </li>

                    <li><strong>Cost Management Across Specialties</strong>
                        <ul>
                            <li>Staff costs by specialty help administrators in budgeting and
                                identifying cost-saving opportunities.</li>
                        </ul>
                    </li>

                    <li><strong>Optimisation and Security Checks</strong>
                        <ul>
                            <li>Identifies employees working across suspicious shifts, showcasing SQL's
                                role in maintaining operational security.</li>
                        </ul>
                    </li>
                </ol>
                <p class="justified-text">In summary, the report demonstrates how SQL can be leveraged for
                    comprehensive
                    data
                    analysis in healthcare, providing actionable insights into staffing, cost
                    management, and security for efficient hospital operations.</p>
                <h3>Relevant Files</h3>
                <ol>
                    <li><strong>Questionnaire</strong>
                        <p class="justified-text"><a href="Documents/Projects/SQL/SQL%20Questionnaire.pdf"
                                target="_blank">Click
                                to
                                Preview</a></p>
                    </li>
                    <li><strong>Report</strong>
                        <p class="justified-text"><a href="Documents/Projects/SQL/Report-28965736.pdf"
                                target="_blank">Click to
                                Preview</a></p>
                    </li>
                    <li><strong>Database</strong>
                        <p class="justified-text"><a href="Documents/Projects/SQL/hospital.db" target="_blank">Click
                                to
                                Preview</a></p>
            </div>
            <!-- </div>
                        </div> -->
            <!-- Project 4: Managing the Transition to Electric Vehicles -->
            <!-- <div class="dropdown" style="position: relative;">
                            <button class="dropdown-btn">Operational Research Case Study: Managing the Transition to
                                Electric Vehicles</button>
                            <div class="dropdown-content"> -->
            <div id="project4" class="box">
                <h3><u>Project 4: Managing the Transition to Electric Vehicles</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The report's primary objective is to examine strategic decisions
                    essential
                    for
                    transitioning from internal combustion engine (ICE) vehicles to electric
                    vehicles
                    (EVs) within the automotive sector, particularly in Southampton. The objectives
                    include:</p>
                <ol class="hanging-indent">
                    <li><strong>Understanding Strategic Considerations</strong>: Analysing the
                        critical
                        decisions that automakers and policymakers face in adopting EVs, including
                        technology development, infrastructure needs, economic impacts, and consumer
                        behavior.</li>
                    <li><strong>Applying Operational Research (OR) Techniques</strong>: Utilising OR
                        and
                        multi-criteria decision-making (MCDM) techniques to evaluate optimal
                        locations
                        for EV infrastructure and to guide policy and business strategy decisions
                        for EV
                        adoption.</li>
                    <li><strong>Developing Optimisation Models for Facility Location</strong>:
                        Creating
                        a mathematical model to decide which petrol stations to maintain for optimal
                        coverage during the shift from ICE vehicles to EVs.</li>
                </ol>
                <p class="justified-text">Overall, the goal is to provide a comprehensive analysis and a
                    mathematical
                    framework
                    to support sustainable and strategic decisions during the transition to EVs,
                    optimising infrastructure and maximising accessibility for remaining ICE
                    vehicles.
                </p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Conceptual Analysis of ICE and EV Technologies</strong>
                        <ul>
                            <li>Historical and functional overview of ICE and EV technologies,
                                assessing
                                their environmental impact, efficiency, infrastructure requirements,
                                and
                                costs.</li>
                            <li>Comparative analysis in a structured table, covering energy
                                efficiency,
                                maintenance, power output, environmental concerns, and global
                                adoption
                                trends.</li>
                        </ul>
                    </li>

                    <li><strong>Mathematical Modeling and Distance Calculation</strong>
                        <ul>
                            <li><strong>Haversine Formula</strong>: Used for calculating
                                geographical
                                distances between petrol stations, accounting for Earth's curvature.
                            </li>
                            <li><strong>Distance Matrix Creation</strong>: Constructed to represent
                                the
                                proximity of petrol stations, serving as foundational data for
                                clustering and optimisation tasks.</li>
                        </ul>
                    </li>

                    <li><strong>Data Analysis and Clustering</strong>
                        <ul>
                            <li><strong>Data Collection</strong>: Geographic coordinates for public
                                petrol stations in Southampton were compiled into a CSV file.</li>
                            <li><strong>K-Means Clustering</strong>: Applied to group petrol
                                stations
                                based on location, identifying high-density areas for EV station
                                prioritisation. Visualisations created with Python's Matplotlib and
                                Seaborn.</li>
                        </ul>
                    </li>

                    <li><strong>Facility Location Optimisation Model</strong>
                        <ul>
                            <li><strong>Decision Variables</strong>: Determine whether a petrol
                                station
                                should remain open (binary variables).</li>
                            <li><strong>Objective Function</strong>: Maximise postcode coverage
                                within a
                                specified radius.</li>
                            <li><strong>Constraints</strong>: Limit open stations to 40% of the
                                total,
                                ensuring strategic coverage.</li>
                            <li><strong>Sensitivity Analysis</strong>: Conducted to determine the
                                optimal coverage radius, showing 15 postcodes covered at a
                                3-kilometer
                                radius.</li>
                        </ul>
                    </li>

                    <li><strong>Alternative Approach and Additional Visualisations</strong>
                        <ul>
                            <li>A secondary model based on map coordinates was considered but set
                                aside
                                for the Haversine distance for accuracy.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Strategic Decisions for EV Transition</strong>
                        <ul>
                            <li>The transition to EVs requires planning across technology,
                                economics,
                                environment, and consumer domains.</li>
                            <li>MCDM techniques, like TOPSIS and AHP, are valuable for
                                infrastructure
                                and strategic planning.</li>
                        </ul>
                    </li>

                    <li><strong>Insights from Facility Location Model</strong>
                        <ul>
                            <li>The model identified an optimal radius of 3 km for petrol station
                                coverage, recommending strategic stations to stay operational.</li>
                            <li>Supports resource-efficient planning for policymakers and
                                businesses.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Integration of Operational Research and AI/ML Techniques</strong>
                        <ul>
                            <li>Combining OR techniques with AI/ML offers powerful decision-support
                                for
                                EV adoption and infrastructure placement.</li>
                            <li>Predictive analytics and reinforcement learning can enhance
                                real-time
                                decision-making for EV infrastructure.</li>
                        </ul>
                    </li>
                </ol>

                <p class="justified-text">In summary, the report emphasises that transitioning to EVs is
                    achievable
                    with
                    operational research and mathematical modeling, supported by AI/ML for
                    adaptable,
                    data-driven strategies in sustainable transportation.</p>
                <h3>Relevant Files</h3>
                <ol>
                    <li><strong>Questionnaire</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Presenting%20Reports/Presenting%20Report%20Brief.pdf"
                                target="_blank">Click to Preview</a></p>
                    </li>
                    <li><strong>Report</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Presenting%20Reports/MATH6145%2028965736.pdf"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
                    <li><strong>PPT</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Presenting%20Reports/MATH6145%2028965736.pptx"
                                target="_blank">Click
                                to Preview</a></p>
                </ol>
            </div>
            <!-- </div>
    </div> -->
            <!-- Project 5: Python Report - Application of Dijkstra's Algorithm -->
            <!-- <div class="dropdown" style="position: relative;">
        <button class="dropdown-btn">Python: Application of Dijkstra's Algorithm</button>
        <div class="dropdown-content"> -->
            <div id="project5" class="box">
                <h3><u>Project 5: Python Report - Application of Dijkstra's Algorithm</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The report primarily aims to explore and apply Dijkstra’s
                    Algorithm within
                    a
                    network
                    setting, focusing on
                    finding the shortest path in a transportation network. The objectives include:
                </p>
                <ol class="hanging-indent">
                    <li><strong>Algorithm Understanding and Application</strong>: To implement
                        Dijkstra’s Algorithm,
                        understanding its mathematical basis, strengths, limitations, and
                        application in
                        graph theory.</li>
                    <li><strong>Shortest Path Computation</strong>: Computing minimum distance paths
                        for
                        a given network,
                        with at least four distinct journeys documented in terms of route and time.
                    </li>
                    <li><strong>Algorithm Validation and Efficiency Analysis</strong>: Testing and
                        validating the
                        algorithm’s correctness against pre-defined benchmarks and evaluating its
                        computational efficiency
                        within the specified network.</li>
                    <li><strong>Documentation and Reusability</strong>: Ensuring clear and detailed
                        documentation for the
                        code to enable understanding, reusability, and adaptation by others,
                        including
                        potential
                        applications in real-world transit systems like Transport for London (TFL).
                    </li>
                </ol>
                <p class="justified-text">The report emphasises both the academic exploration of Dijkstra’s
                    Algorithm
                    and
                    its
                    practical
                    applicability in solving transportation and network optimisation challenges.</p>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Theoretical Framework of Dijkstra’s Algorithm</strong>
                        <ul>
                            <li>The report covers the background, core principles, and properties of
                                Dijkstra’s Algorithm,
                                detailing its efficiency in solving the single-source shortest path
                                problem in weighted
                                graphs.</li>
                            <li>Fundamental concepts from graph theory, such as vertices, edges,
                                paths,
                                and cycles, are
                                introduced to contextualise the algorithm’s operation and relevance.
                            </li>
                            <li>Key properties like the algorithm’s greedy nature, optimality in
                                networks with non-negative
                                weights, and its inability to handle negative cycles are discussed.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Code Implementation</strong>
                        <ul>
                            <li><strong>Algorithm Development</strong>: A Python script was created
                                to
                                implement Dijkstra’s
                                Algorithm, utilising a priority queue to keep track of shortest
                                paths
                                from a start node to
                                all other nodes in a graph.</li>
                            <li><strong>Graph Representation</strong>: The network was represented
                                as a
                                dictionary, where
                                keys are nodes (stations), and values are dictionaries of
                                neighboring
                                nodes with respective
                                distances.</li>
                            <li><strong>Path Calculation</strong>: For each starting station, the
                                algorithm computed
                                shortest paths to other nodes, tracking the cumulative distance and
                                sequence of nodes
                                visited.</li>
                            <li><strong>Modular Function Design</strong>: Functions such as
                                `dijkstra`
                                for the main
                                algorithm and `get_path` for path reconstruction were created to
                                structure and simplify the
                                code.</li>
                        </ul>
                    </li>

                    <li><strong>Testing and Output</strong>
                        <ul>
                            <li><strong>Testing Scenarios</strong>: The code was tested across a
                                London
                                rail network, with
                                specific routes like Paddington to Charing Cross, Victoria to
                                Liverpool
                                Street, Embankment
                                to Kings Cross, and London Bridge to Oxford Circus.</li>
                            <li><strong>Output Verification</strong>: Results for each route,
                                including
                                total travel time
                                and path, were documented, verifying against expected results and
                                manually reviewing paths.
                            </li>
                            <li><strong>Sanity Check</strong>: The algorithm was tested on a smaller
                                graph network to
                                confirm initial functionality.</li>
                        </ul>
                    </li>

                    <li><strong>Collaborative Development and Meetings</strong>
                        <ul>
                            <li>Six structured meetings were held to focus on knowledge sharing,
                                code
                                development, testing,
                                and report documentation.</li>
                            <li>Key activities included collaborative testing, debugging, and final
                                report refinement.</li>
                            <li>A shared document on Google Docs enabled real-time collaboration on
                                the
                                report.</li>
                        </ul>
                    </li>

                    <li><strong>Documentation for Accessibility and Reusability</strong>
                        <ul>
                            <li>Comprehensive documentation was provided within the code, making the
                                algorithm accessible
                                even to those unfamiliar with Python.</li>
                            <li>Steps for modifying start and end stations are clearly outlined for
                                adaptation to different
                                networks.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Algorithm Reliability and Accuracy</strong>
                        <ul>
                            <li>The implemented algorithm successfully computed accurate shortest
                                paths
                                within the specified
                                network, demonstrating robustness and precision across various
                                scenarios.</li>
                        </ul>
                    </li>

                    <li><strong>Code Documentation and Usability</strong>
                        <ul>
                            <li>Documentation was emphasised, making the code accessible to
                                individuals
                                without extensive
                                Python expertise.</li>
                            <li>The design is well-suited for adaptation by organisations like TFL.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Potential for Real-World Application</strong>
                        <ul>
                            <li>The report notes Dijkstra’s Algorithm's potential in supporting
                                network
                                optimisation tasks
                                in practical settings, such as urban transportation networks.</li>
                        </ul>
                    </li>

                    <li><strong>Group Collaboration and Project Reflection</strong>
                        <ul>
                            <li>The collaborative effort contributed to the project’s success,
                                ensuring
                                thorough testing and
                                polished documentation.</li>
                        </ul>
                    </li>
                </ol>

                <p class="justified-text">In summary, the report affirms Dijkstra’s Algorithm as an
                    effective
                    solution
                    for
                    shortest path problems
                    in transportation networks, with a robust, reusable code implementation
                    supported by
                    rigorous testing
                    and comprehensive documentation.</p>
                <h3>Relevant Files</h3>
                <ol>
                    <li><strong>Questionnaire</strong>
                        <p class="justified-text"><a href="Documents/Projects/Python%20Dijkstra/MATH6005_Coursework.pdf"
                                target="_blank">Click
                                to
                                Preview</a></p>
                    </li>
                    <li><strong>Report</strong>
                        <p class="justified-text"><a href="Documents/Projects/Python%20Dijkstra/Python%20Report.pdf"
                                target="_blank">Click
                                to
                                Preview</a></p>
                    </li>
                    <li><strong>Python Code</strong>
                        <p class="justified-text"><a href="Documents/Projects/Python%20Dijkstra/Dijkstra_Code.py"
                                target="_blank">Click to
                                Preview</a></p>
            </div>
            <!-- </div>
    </div> -->
            <!-- Project 6: MATH6119 PCB Assembly Case Study for Philips Electronics -->
            <!-- <div class="dropdown" style="position: relative;">
        <button class="dropdown-btn">PCB Assembly Case Study for Philips Electronics</button>
        <div class="dropdown-content"> -->
            <div id="project6" class="box">
                <h3><u>Project 6: PCB Assembly Case Study for Philips Electronics</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The case study aims to develop an optimised solution for Philips
                    Electronics
                    to enhance the efficiency of
                    its printed circuit board (PCB) assembly system. Key objectives include:
                </p>
                <ol class="hanging-indent">
                    <li><strong>Optimising Assembly Throughput:</strong>Increase production
                        speed by minimising the time
                        required for each PCB assembly.</li>
                    <li><strong>Balancing Workload Across Machines:</strong>Distribute tasks
                        among machines to prevent
                        bottlenecks and ensure smoother production flow.</li>
                    <li><strong>Minimising Robot Arm Movement:</strong>Optimise
                        pick-and-place
                        processes to reduce robot
                        arm travel distances, impacting cycle times.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Understanding the Assembly System</strong>
                        <ul>
                            <li>Examined machine roles and dependencies in Philips’ assembly
                                line, recognising synchronised
                                cycle times.</li>
                        </ul>
                    </li>

                    <li><strong>Data Analysis and Component Allocation</strong>
                        <ul>
                            <li>Mapped components to machines, optimised pick-and-place
                                sequences, and minimised travel
                                distances.</li>
                        </ul>
                    </li>

                    <li><strong>Modeling Machine Throughput and Workload
                            Distribution</strong>
                        <ul>
                            <li>Calculated cycle time and balanced workloads by reallocating
                                tasks to reduce cycle times.
                            </li>
                        </ul>
                    </li>

                    <li><strong>Throughput Maximisation Strategy</strong>
                        <ul>
                            <li>Implemented strategies to minimise bottlenecks through task
                                reallocation and travel distance
                                reductions.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Increased Throughput</strong>: Enhanced throughput through
                        optimised travel paths and task
                        distribution.</li>
                    <li><strong>Balanced Machine Utilisation</strong>: Achieved consistent
                        cycle
                        times by evenly
                        distributing machine workloads.</li>
                    <li><strong>Strategic Task Allocation</strong>: Recommended minimising
                        movement with optimised
                        pick-and-place sequences.</li>
                    <li><strong>Recommendations for Implementation</strong>: Provided steps
                        for
                        implementing optimised paths
                        and configurations with minimal disruption.</li>
                </ol>

                <h3>Relevant Files</h3>
                <ol>
                    <li>
                        <strong>Questionnaire</strong>
                        <p class="justified-text"><a href="Documents/Projects/MATH6119_PCB/MATH6119%20Brief.pdf"
                                target="_blank">Click
                                to
                                Preview</a></p>
                    </li>
                    <li>
                        <strong>PPT</strong>
                        <p class="justified-text"><a href="Documents/Projects/MATH6119_PCB/MATH6119_PPT.pptx"
                                target="_blank">Click
                                to Preview</a>
                        </p>
                    </li>
                    <li>
                        <strong>Supporting Files</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/MATH6119_PCB/GroupNumberMachinesTemplate.zip"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
            </div>
            <!-- </div>
    </div> -->
            <!-- Project 7: MATH6145 Fiscal Impact of COVID-19 on the UK -->
            <!-- <div class="dropdown" style="position: relative;">
        <button class="dropdown-btn">Presentation on the Fiscal Impact of COVID-19 on the UK</button>
        <div class="dropdown-content"> -->
            <div id="project7" class="box">
                <h3><u>Project 7: Fiscal Impact of COVID-19 on the UK</u></h3>
                <h3>Objective</h3>
                <p class="justified-text">The primary objective of this study is to examine the fiscal,
                    accounting,
                    and healthcare impacts of
                    COVID-19 on the UK. The objectives include:</p>
                <ol class="hanging-indent">
                    <li><strong>Analysing Government Budgetary and Fiscal
                            Responses</strong>: Exploring impacts on UK public
                        finances through various accounting methods.</li>
                    <li><strong>Assessing Data Analytics in Healthcare</strong>:
                        Highlighting contributions of data
                        analytics in pandemic management.</li>
                    <li><strong>Evaluating Long-Term Fiscal Implications</strong>:
                        Analysing
                        long-term impacts on fiscal
                        sustainability and public finance.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Government Accounting Analysis</strong>
                        <ul>
                            <li>Examined four accounting modes (statistical, budgeting,
                                financial reporting, fiscal
                                sustainability).</li>
                        </ul>
                    </li>

                    <li><strong>Healthcare Data Analytics and Operations
                            Research</strong>
                        <ul>
                            <li>Used data analytics for diagnostics, resource
                                allocation,
                                and predictive modeling.</li>
                        </ul>
                    </li>

                    <li><strong>Case Studies of Healthcare Optimisation</strong>
                        <ul>
                            <li>Analysed patient allocation and outbreak prediction
                                using
                                data-driven models.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Fiscal Impacts</strong>: Noted budget deficit increases
                        and
                        long-term fiscal challenges.
                    </li>
                    <li><strong>Healthcare Implications of Data Analytics</strong>:
                        Showed
                        enhanced healthcare resource
                        allocation and management.</li>
                    <li><strong>Policy Recommendations</strong>: Suggested policies for
                        fiscal resilience and improved
                        healthcare data infrastructure.</li>
                </ol>

                <h3>Relevant Files</h3>
                <ol>
                    <li><strong>Questionnaire</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Presenting%20Reports/Presenting%20Report%20Brief.pdf"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
                    <li><strong>Report</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Presenting%20Reports/MATH6145%2028965736.pdf"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
                    <li>
                        <strong>PPT</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Presenting%20Reports/MATH6145%2028965736.pptx"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
                </ol>
            </div>
            <!-- </div>
                                            </div> -->
            <!-- Project 8: Revenue Management Case Study: Carnival Cruise Line -->
            <!-- <div class="dropdown" style="position: relative;">
                                            <button class="dropdown-btn">Revenue Management Case Study: Carnival Cruise Line</button>
                                            <div class="dropdown-content"> -->
            <div id="project8" class="box">
                <h3><u>Project 8: Revenue Management Case Study: Carnival Cruise
                        Line</u>
                </h3>
                <h3>Objective</h3>
                <p class="justified-text">The objective is to apply revenue management principles to
                    Carnival
                    Cruise Line, aiming to maximise
                    revenue for 2025. Objectives include:</p>
                <ol class="hanging-indent">
                    <li><strong>Forecasting Demand</strong>: Estimating demand for
                        short and
                        long routes for 2025.</li>
                    <li><strong>Revenue Optimisation with Linear
                            Programming</strong>: Using
                        LP to optimise cabin allocation
                        for maximum revenue.</li>
                    <li><strong>Overbooking and Cancellation Analysis</strong>:
                        Managing
                        overbooking based on historical
                        data.</li>
                    <li><strong>Scenario-Based Decision Making</strong>: Developing
                        scenarios for market growth and
                        price-sensitive strategies.</li>
                </ol>

                <h3>Methodology</h3>
                <ol>
                    <li><strong>Data Analysis and Demand Estimation</strong>
                        <ul>
                            <li>Analysed WorldCruiseData2024.csv for trends and
                                booking
                                patterns.</li>
                        </ul>
                    </li>

                    <li><strong>Revenue Maximisation Model</strong>
                        <ul>
                            <li>LP model to allocate cabins optimally under
                                different demand
                                scenarios.</li>
                        </ul>
                    </li>

                    <li><strong>Cancellation and Overbooking Analysis</strong>
                        <ul>
                            <li>Analysed cancellation rates and proposed overbooking
                                strategies.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Conclusion</h3>
                <ol>
                    <li><strong>Scenario Findings</strong>: Provided insights into
                        demand
                        and revenue under market growth
                        and aggressive marketing scenarios.</li>
                    <li><strong>Overbooking Strategies</strong>: Recommended
                        overbooking and
                        pricing adjustments.</li>
                    <li><strong>Strategic Recommendations</strong>: Suggested
                        tailored cabin
                        promotions, flexible pricing,
                        and scenario-based marketing.</li>
                </ol>

                <ol>
                    <h3>Relevant Files</h3>
                    <li>
                        <strong>Questionnaire</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Revenue_Management/MATH6146%20Case%20Study%202024.docx"
                                target="_blank">Click
                                to Preview</a></p>
                    </li>
                    <li>
                        <strong>Presentation</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Revenue_Management/Revenue%20Management%20-%20Group%20Presentation.pptx"
                                target="_blank">Click to Preview</a></p>
                    </li>
                    <li>
                        <strong>Working Files</strong>
                        <p class="justified-text"><a
                                href="Documents/Projects/Revenue_Management/WorldCruiseData2024.csv"
                                target="_blank">Click
                                to
                                Preview</a></p>
                    </li>
                </ol>
            </div>
            <!-- </div>
            </div> -->
            <!-- <div class="dropdown" style="position: relative;">
                                                <button class="dropdown-btn">Autonomous Staircase Climbing Robot for
                                                    Rescue Operation</button>
                                                <div class="dropdown-content"> -->
            <div id="project9" class="box">
                <h3><u>Project 9: Autonomous Staircase Climbing Robot for Rescue
                        Operation
                    </u></h3>
                <p class="justified-text">Research Paper</p>
                <a href="https://iopscience.iop.org/article/10.1088/1757-899X/912/3/032085/pdf">Link</a>
            </div>
        </section>
        <!-- </div>
                                            </div> -->

        <section id="education">
            <h2 class="section-title">Education</h2>
            <!-- Master's Degree -->
            <div class="degree-card">
                <h4>Master's</h4>
                <p class="institution">University of Southampton - Southampton, United Kingdom</p>
                <p class="dates">September 2023 - September 2024</p>
                <p class="program">Master's of Science - Mathematics - Data and Decision Analytics</p>
                <p class="modules italic-text">Key Modules: Data Analytics, Data Science Techniques of Python,
                    Snowflake, Statistical Computing for Data Science, Machine Learning, SQL, Power BI Data
                    Visualisations, and Deterministic Methods for Data Science</p>
            </div>

            <!-- Bachelor's Degree -->
            <div class="degree-card">
                <h4>Bachelor's</h4>
                <p class="institution">SRM Institute of Science and Technology - Chennai, India</p>
                <p class="dates">July 2016 - December 2020</p>
                <p class="program">Bachelor's of Technology - Mechanical Engineering</p>
            </div>
        </section>

        <section id="contact">
            <h2 class="section-title">Contact</h2>
            <div class="box">
                <div class="contact-details">
                    <p class="justified-text"><span class="contact-icon">📧</span> Mail:
                        <a href="mailto:vyomkhanna355@gmail.com">vyomkhanna355@gmail.com</a>
                    </p>
                    <p class="justified-text"><span class="contact-icon">📱</span> Call or WhatsApp:
                        <a href="tel:+447733989297">+44 7733 989297</a>
                    </p>
                    <p class="justified-text"><span class="contact-icon">💬</span> WhatsApp:
                        <a href="https://wa.me/447733989297" target="_blank">Chat on WhatsApp</a>
                    </p>
                    <p class="justified-text"><span class="contact-icon">💼</span> LinkedIn:
                        <a href="https://www.linkedin.com/in/vyom-khanna/"
                            target="_blank">linkedin.com/in/vyom-khanna</a>
                    </p>
                </div>
            </div>
        </section>

        <section id="resume">
            <h2 class="section-title">Resume and Cover</h2>
            <p class="justified-text">View my resume below or <a
                    href="Documents/Resume%20and%20Cover/Vyom_Khanna_CV.pdf" target="_blank">open
                    in a
                    new tab</a>.</p>
            <iframe src="Documents/Resume%20and%20Cover/Vyom_Khanna_CV.pdf" width="100%" height="600px"
                style="border: none;">
            </iframe>
            <br>
            <p class="justified-text">Click the link below to view my cover letter:</p>
            <p class="justified-text"><a href="Documents/Resume%20and%20Cover/vyom_Khanna_Cover.pdf"
                    target="_blank">Open
                    Cover Letter</a>
            </p>
        </section>

        <section id="certificates">
            <h2 class="section-title">Certificates</h2>
            <div class="box">
                <h3>Letter Of Recommendations</h3>
                <p class="justified-text">View my Letter of Recommendation from SRM Insititue of Science and Technolgy
                    <a href="Documents/Certificates_and_LOR/VyomKhanna_LOR_SRM.pdf" target="_blank">open in a new
                        tab</a>.
                </p>
                <p class="justified-text">View my Letter of Recommendation from on-site Client - Care Health Insurance
                    <a href="Documents/Certificates_and_LOR/Vyom_Khanna_LOR_CHI.pdf" target="_blank">open in a new
                        tab</a>.
                </p>

                <h3>University of Southampton - Transcript</h3>
                <p class="justified-text"><a href="Documents/Certificates_and_LOR/UOS_Transcript.pdf"
                        target="_blank">Transcript</a></p>

                <h3>Online Courses - Certificates</h3>
                <p class="justified-text"><a href="Documents/Certificates_and_LOR/SQL%20Bootcamp.pdf"
                        target="_blank">SQL
                        Bootcamp </a></p>
                <p class="justified-text"><a href="Documents/Certificates_and_LOR/Excel.pdf" target="_blank">Microsoft
                        Excel</a></p>
                <p class="justified-text"><a href="Documents/Certificates_and_LOR/Python Bootcamp.pdf"
                        target="_blank">Python Bootcamp</a></p>
            </div>
        </section>


        <section id="mobility">
            <h2 class="section-title">Mobility</h2>
            <div class="box">
                <p class="justified-text">I've heard the feedback, yes, I’m currently in Southampton. But here’s the
                    deal:
                    I’m here temporarily,
                    just until I land a job. Having relocated from India to the UK, I'm more than ready to move again
                    for the right opportunity.

                    I didn’t realise location might raise eyebrows in the job search, but no worries! I’m happy to
                    relocate without any relocation package. Manchester, Bristol, London? I’m there. For other areas,
                    just give me a week to pack my bags!</p>
            </div>
        </section>

        <section id="right_to_work">
            <h2 class="section-title">Right to Work</h2>
            <div class="box">
                <p class="justified-text">
                    The classic “Right to Work” question, let’s talk about it.
                    I’m currently on a Graduate Visa, valid until 23rd January 2027. Beyond that, I would require visa
                    sponsorship to continue contributing my skills and expertise in the UK.</p>
                <p class="justified-text">
                    That said, I firmly believe sponsorship should feel like a smart investment, not a favour. With my
                    strong work ethic, commitment to excellence, and results-driven mindset, my goal is to become a
                    Subject Matter Expert (SME) within the next six months. I’m focused on deeply understanding my
                    stakeholders, building trust with my team, and gradually expanding my scope to support other
                    departments.</p>
                <p class="justified-text">
                    By the time 2027 arrives, I want to be seen not just as a high-performing team member, but as a
                    valuable asset to the organisation, someone you’d be proud (and eager) to sponsor.

                    Challenge accepted.
                </p>
            </div>
        </section>

        <!-- <section id="More">
            <h2 class="section-title">Lastly...</h2>
            <div class="box">
                <p class="justified-text">
                    After browsing through this page, you may have some reservations perhaps about my visa status,
                    UK-based
                    experience, or industry alignment. Let me assure you that my strong work ethic, adaptability, and
                    eagerness to learn will bridge any perceived gaps.
                    If you’re willing to take a leap of faith, I will prioritise proving that it was the right choice.
                </p>
                <br>
                <p class="justified-text">
                    Although I am in the early stages of my career, I bring a solid foundation built through numerous
                    projects that have honed my ability to excel in any industry. I am confident that with just a couple
                    of
                    months of immersion, I can seamlessly adapt to a new work environment, office routines, compliance
                    requirements, and team dynamics. My background in healthcare insurance has given me valuable
                    insights
                    into the importance of accuracy, process integrity, and maintaining high standards qualities I carry
                    with me into every endeavour.
                </p>
                <br>
                <p class="justified-text">
                    I have also taken the initiative to familiarise myself with UK GDPR / Data Protection Act 2018 rules
                    and
                    regulations, and my
                    experience in healthcare has instilled in me a deep understanding of the critical importance of data
                    integrity and confidentiality. While I’m eager to explore opportunities across various sectors, I’m
                    confident that my skills and commitment to excellence will add significant value to any team.
                    If given the opportunity, I’ll bring fresh perspectives, a strong drive to contribute, and the
                    ability
                    to quickly adapt to the organisation’s culture and goals. I would be delighted to discuss how I can
                    contribute to your success.
                </p>
            </div>
        </section> -->

        <!-- <script>
            // Select all dropdown buttons
            document.querySelectorAll('.dropdown-btn').forEach(button => {
                button.addEventListener('click', (event) => {
                    // Prevent the click from bubbling to the document click listener
                    event.stopPropagation();

                    // Toggle the clicked dropdown content
                    const dropdownContent = button.nextElementSibling;
                    const isVisible = dropdownContent.style.display === 'block';

                    // Close all other dropdowns
                    document.querySelectorAll('.dropdown-content').forEach(content => {
                        content.style.display = 'none';
                    });

                    // Toggle the visibility of the clicked dropdown
                    dropdownContent.style.display = isVisible ? 'none' : 'block';
                });
            });

            // Close dropdowns if clicked outside
            document.addEventListener('click', () => {
                document.querySelectorAll('.dropdown-content').forEach(content => {
                    content.style.display = 'none';
                });
            });
        </script> -->
        <script>
            // Select the toggle button, navigation drawer, and main container
            const navToggle = document.getElementById("nav-toggle");
            const navDrawer = document.getElementById("nav-drawer");

            // Open/close the navigation drawer on toggle button click
            navToggle.addEventListener("click", () => {
                navDrawer.classList.toggle("open");
            });

            // Close the drawer when a link is clicked
            const navLinks = document.querySelectorAll(".nav-drawer ul li a");
            navLinks.forEach(link => {
                link.addEventListener("click", () => {
                    navDrawer.classList.remove("open");
                });
            });

            // Close the drawer when clicking outside of it
            document.addEventListener("click", event => {
                const isClickInside = navDrawer.contains(event.target) || navToggle.contains(event.target);
                if (!isClickInside) {
                    navDrawer.classList.remove("open");
                }
            });

            // Close the drawer when scrolling to a section
            document.addEventListener("scroll", () => {
                if (navDrawer.classList.contains("open")) {
                    navDrawer.classList.remove("open");
                }
            }); 
        </script>

    </div>
</body>

</html>